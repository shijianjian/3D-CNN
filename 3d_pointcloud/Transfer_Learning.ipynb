{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data comes from http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html\n",
    "from pyntcloud import PyntCloud\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if sys.platform == 'darwin':\n",
    "    data_path = os.getcwd() + \"/PartAnnotation\"\n",
    "else:\n",
    "    data_path = os.getcwd() + \"\\\\PartAnnotation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_var_names():\n",
    "    all_vars = []\n",
    "    for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "        all_vars.append(i.name)\n",
    "    return all_vars\n",
    "\n",
    "def get_all_placeholders():\n",
    "    return [x for x in tf.get_default_graph().get_operations() if x.type == \"Placeholder\"]\n",
    "\n",
    "def get_all_mean_op():\n",
    "    return [x for x in tf.get_default_graph().get_operations() if x.type == \"Mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1/kernel:0', 'conv1/bias:0', 'conv2/kernel:0', 'conv2/bias:0', 'conv4/kernel:0', 'conv4/bias:0', 'conv5/kernel:0', 'conv5/bias:0', 'conv7/kernel:0', 'conv7/bias:0', 'conv8/kernel:0', 'conv8/bias:0', 'bn/beta:0', 'bn/gamma:0', 'bn/moving_mean:0', 'bn/moving_variance:0', 'full_con/kernel:0', 'full_con/bias:0', 'y_pred/kernel:0', 'y_pred/bias:0', 'training/beta1_power:0', 'training/beta2_power:0', 'conv1/kernel/Adam:0', 'conv1/kernel/Adam_1:0', 'conv1/bias/Adam:0', 'conv1/bias/Adam_1:0', 'conv2/kernel/Adam:0', 'conv2/kernel/Adam_1:0', 'conv2/bias/Adam:0', 'conv2/bias/Adam_1:0', 'conv4/kernel/Adam:0', 'conv4/kernel/Adam_1:0', 'conv4/bias/Adam:0', 'conv4/bias/Adam_1:0', 'conv5/kernel/Adam:0', 'conv5/kernel/Adam_1:0', 'conv5/bias/Adam:0', 'conv5/bias/Adam_1:0', 'conv7/kernel/Adam:0', 'conv7/kernel/Adam_1:0', 'conv7/bias/Adam:0', 'conv7/bias/Adam_1:0', 'conv8/kernel/Adam:0', 'conv8/kernel/Adam_1:0', 'conv8/bias/Adam:0', 'conv8/bias/Adam_1:0', 'bn/beta/Adam:0', 'bn/beta/Adam_1:0', 'bn/gamma/Adam:0', 'bn/gamma/Adam_1:0', 'full_con/kernel/Adam:0', 'full_con/kernel/Adam_1:0', 'full_con/bias/Adam:0', 'full_con/bias/Adam_1:0', 'y_pred/kernel/Adam:0', 'y_pred/kernel/Adam_1:0', 'y_pred/bias/Adam:0', 'y_pred/bias/Adam_1:0']\n",
      "\n",
      "Optimizer [<tf.Operation 'training/Adam' type=NoOp>]\n",
      "\n",
      "Placeholders [<tf.Operation 'inputs/x_input' type=Placeholder>, <tf.Operation 'inputs/y_input' type=Placeholder>]\n",
      "\n",
      "Mean [<tf.Operation 'cross_entropy/batch_norm/bn/moments/Mean' type=Mean>, <tf.Operation 'cross_entropy/cost' type=Mean>, <tf.Operation 'acc' type=Mean>]\n"
     ]
    }
   ],
   "source": [
    "new_graph = tf.Graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True) # allow passing gpu-trained model to a cpu machine\n",
    "with tf.Session(graph=new_graph, config=config) as sess:\n",
    "\n",
    "    import os\n",
    "    model_fqn = os.path.join(os.getcwd(), 'trained_model', \"model-4\")\n",
    "    saver = tf.train.import_meta_graph(model_fqn + \".meta\")\n",
    "    saver.restore(sess, model_fqn)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    print(get_all_var_names())\n",
    "    print(\"\\nOptimizer\", graph.get_collection(\"optimizer\"))\n",
    "    print(\"\\nPlaceholders\", get_all_placeholders())\n",
    "    print(\"\\nMean\", get_all_mean_op())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def voxelize3D(pts, dim=[1,1,1]):\n",
    "    \"\"\"\n",
    "    pts: receives .pts cloud point data. 2D array, arbitary sized X,Y,Z pairs. (We will only take x,y,z into account for now)\n",
    "    dim: dimensioin of output voxelized data\n",
    "    \n",
    "    This function will locate the grid cube and calculate the density of each cube.\n",
    "    The output will be normalized values.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1]>=3), \"pts file should contain at least x,y,z coordinate\"\n",
    "    assert(len(dim)==3), \"Please provide 3-d grid size like [32,32,32]\"\n",
    "    \n",
    "    # move all the axis to positive area.\n",
    "    minimum_val = [pts[0][0], pts[0][1], pts[0][2]]\n",
    "\n",
    "    # find the smallest \n",
    "    for pair in pts:\n",
    "        if pair[0] < minimum_val[0]:\n",
    "            minimum_val[0] = pair[0]\n",
    "        if pair[1] < minimum_val[1]:\n",
    "            minimum_val[1] = pair[1]\n",
    "        if pair[2] < minimum_val[2]:\n",
    "            minimum_val[2] = pair[2]\n",
    "            \n",
    "    # move it to first quadrant \n",
    "    rectified_pts = np.empty(pts.shape)\n",
    "    for index, pair in enumerate(pts):\n",
    "        point = np.zeros(3)\n",
    "        point[0] = pair[0] - minimum_val[0]\n",
    "        point[1] = pair[1] - minimum_val[1]\n",
    "        point[2] = pair[2] - minimum_val[2]\n",
    "        rectified_pts[index] = point\n",
    "    \n",
    "    # biggest value in each axis \n",
    "    maximum_val = pts[0][0]\n",
    "    \n",
    "    for pair in rectified_pts:\n",
    "        for val in pair:\n",
    "            if val > maximum_val:\n",
    "                maximum_val = val\n",
    "     \n",
    "    # normalize all the axises to (0,1)\n",
    "    normalized_pts = rectified_pts/maximum_val\n",
    "    \n",
    "    x_grid_length = 1/dim[0]\n",
    "    y_grid_length = 1/dim[1]\n",
    "    z_grid_length = 1/dim[2]\n",
    "    \n",
    "    output = np.zeros((dim[0],dim[1],dim[2]))\n",
    "    \n",
    "    epsilon = 0.000000000001 # we will have at least a 1.0 value which will exceed the index of grid\n",
    "    # we can use a relativly small value to escape that to fit our data\n",
    "    \n",
    "    max_volume_size = 0\n",
    "    \n",
    "    for pair in normalized_pts:\n",
    "        x_loc = int(pair[0]/(x_grid_length + epsilon))\n",
    "        y_loc = int(pair[1]/(y_grid_length + epsilon))\n",
    "        z_loc = int(pair[2]/(z_grid_length + epsilon))\n",
    "        if output[x_loc, y_loc, z_loc] is None:\n",
    "            output[x_loc, y_loc, z_loc] = 1\n",
    "        else:\n",
    "            output[x_loc, y_loc, z_loc] += 1\n",
    "        \n",
    "        if output[x_loc, y_loc, z_loc] > max_volume_size:\n",
    "            max_volume_size = output[x_loc, y_loc, z_loc]\n",
    "    \n",
    "    output = output/max_volume_size    \n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data_path, max_file_num=None, dim=[32,32,32]):\n",
    "    data = []\n",
    "    \n",
    "    target_dir_path = os.path.join(data_path, 'points')\n",
    "    path, dirs, files = os.walk(target_dir_path).__next__()\n",
    "    file_count = len(files)\n",
    "    \n",
    "    count = 0\n",
    "    for pts_data in os.scandir(target_dir_path):\n",
    "        if (max_file_num is None) or (count < max_file_num):\n",
    "            _path = os.path.join(data_path, 'points', pts_data.name)\n",
    "            pts = PyntCloud.from_file(_path, sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "            _vox = voxelize3D(pts.xyz, dim=dim)\n",
    "            vox_chan = np.array(_vox).reshape(_vox.shape + (1,))\n",
    "            data.append(vox_chan)\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv3d(_input, w, b, layer=None):\n",
    "    return tf.nn.relu(tf.nn.conv3d(_input, w, strides=[1, 1, 1, 1, 1], padding=\"SAME\", name=\"conv\" + str(layer)) + b, name=\"relu\" + str(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(model_path):\n",
    "    \n",
    "    new_graph = tf.Graph()\n",
    "    config = tf.ConfigProto(allow_soft_placement=True) # allow passing gpu-trained model to a cpu machine\n",
    "    \n",
    "    with tf.Session(graph=new_graph, config=config) as sess:\n",
    "        \n",
    "        # Restore model\n",
    "        saver = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        model_graph = tf.get_default_graph()\n",
    "        \n",
    "        _conv1_w = model_graph.get_tensor_by_name('conv1/kernel:0')\n",
    "        _conv1_b = model_graph.get_tensor_by_name('conv1/bias:0')   \n",
    "\n",
    "        _conv2_w = model_graph.get_tensor_by_name('conv2/kernel:0')\n",
    "        _conv2_b = model_graph.get_tensor_by_name('conv2/bias:0')  \n",
    "\n",
    "        _conv4_w = model_graph.get_tensor_by_name('conv4/kernel:0')\n",
    "        _conv4_b = model_graph.get_tensor_by_name('conv4/bias:0')\n",
    "\n",
    "        _conv5_w = model_graph.get_tensor_by_name('conv5/kernel:0')\n",
    "        _conv5_b = model_graph.get_tensor_by_name('conv5/bias:0')\n",
    "        \n",
    "        _conv7_w = model_graph.get_tensor_by_name('conv7/kernel:0')\n",
    "        _conv7_b = model_graph.get_tensor_by_name('conv7/bias:0')\n",
    "        \n",
    "        _conv8_w = model_graph.get_tensor_by_name('conv8/kernel:0')\n",
    "        _conv8_b = model_graph.get_tensor_by_name('conv8/bias:0')\n",
    "    \n",
    "        _dict = {\n",
    "            'conv1_w': _conv1_w.eval(),\n",
    "            'conv1_b': _conv1_b.eval(),\n",
    "            'conv2_w': _conv2_w.eval(),\n",
    "            'conv2_b': _conv2_b.eval(),\n",
    "            'conv4_w': _conv4_w.eval(),\n",
    "            'conv4_b': _conv4_b.eval(),\n",
    "            'conv5_w': _conv5_w.eval(),\n",
    "            'conv5_b': _conv5_b.eval(),\n",
    "            'conv7_w': _conv7_w.eval(),\n",
    "            'conv7_b': _conv7_b.eval(),\n",
    "            'conv8_w': _conv8_w.eval(),\n",
    "            'conv8_b': _conv8_b.eval(),\n",
    "        }\n",
    "\n",
    "    return _dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn3d_model_with_default(params, x_input, output_label_size, keep_rate=0.5):\n",
    "    \n",
    "    with tf.name_scope('conv1_layer'):\n",
    "        conv1_w = tf.Variable(initial_value=params['conv1_w'], name='w')\n",
    "        conv1_b = tf.Variable(initial_value=params['conv1_b'], name='b')\n",
    "\n",
    "        conv1 = conv3d(x_input, conv1_w, conv1_b, 1)\n",
    "\n",
    "    with tf.name_scope('conv2_layer'):\n",
    "        conv2_w = tf.Variable(initial_value=params['conv2_w'], name='w')\n",
    "        conv2_b = tf.Variable(initial_value=params['conv2_b'], name='b')\n",
    "\n",
    "        conv2 = conv3d(conv1, conv2_w, conv2_b, 2)\n",
    "\n",
    "    with tf.name_scope('pool3_layer'):\n",
    "        pool3 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[2, 2, 2], strides=2, name=\"pool3\")\n",
    "\n",
    "\n",
    "    with tf.name_scope('conv4_layer'):\n",
    "        conv4_w = tf.Variable(initial_value=params['conv4_w'], name='w')\n",
    "        conv4_b = tf.Variable(initial_value=params['conv4_b'], name='b')\n",
    "      \n",
    "        conv4 = conv3d(pool3, conv4_w, conv4_b, 4)\n",
    "        \n",
    "    with tf.name_scope('conv5_layer'):\n",
    "        conv5_w = tf.Variable(initial_value=params['conv5_w'], name='w')\n",
    "        conv5_b = tf.Variable(initial_value=params['conv5_b'], name='b')\n",
    "      \n",
    "        conv5 = conv3d(conv4, conv5_w, conv5_b, 5)\n",
    "        \n",
    "    with tf.name_scope('pool6_layer'):\n",
    "        pool6 = tf.layers.max_pooling3d(inputs=conv5, pool_size=[2, 2, 2], strides=2, name=\"pool6\")\n",
    "        \n",
    "    with tf.name_scope('conv7_layer'):\n",
    "        conv7_w = tf.Variable(initial_value=params['conv7_w'], name='w')\n",
    "        conv7_b = tf.Variable(initial_value=params['conv7_b'], name='b')\n",
    "      \n",
    "        conv7 = conv3d(pool6, conv7_w, conv7_b, 7)\n",
    "        \n",
    "    with tf.name_scope('conv8_layer'):\n",
    "        conv8_w = tf.Variable(initial_value=params['conv8_w'], name='w')\n",
    "        conv8_b = tf.Variable(initial_value=params['conv8_b'], name='b')\n",
    "      \n",
    "        conv8 = conv3d(conv7, conv8_w, conv8_b, 8)\n",
    "        \n",
    "    with tf.name_scope('pool9_layer'):\n",
    "        pool9 = tf.layers.max_pooling3d(inputs=conv8, pool_size=[2, 2, 2], strides=2, name=\"pool9\")\n",
    "           \n",
    "    with tf.name_scope(\"batch_norm\"):\n",
    "        cnn3d_bn = tf.layers.batch_normalization(inputs=pool9, training=True, name=\"bn\")\n",
    "        \n",
    "    with tf.name_scope(\"fully_con\"):\n",
    "        flattening = tf.reshape(cnn3d_bn, [-1, 4*4*4*512])\n",
    "        dense = tf.layers.dense(inputs=flattening, units=1024, activation=tf.nn.relu, name=\"full_con\")\n",
    "        # (1-keep_rate) is the probability that the node will be kept\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=keep_rate, training=True, name=\"dropout\")\n",
    "        \n",
    "    with tf.name_scope(\"y_conv\"):\n",
    "        y_conv = tf.layers.dense(inputs=dropout, units=output_label_size, name=\"y_pred\") \n",
    "        \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "[0.0]\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# one hot indexes\n",
    "# 2: chair , 3: car\n",
    "\n",
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'model', 'model-0')\n",
    "\n",
    "params = get_model_params(model_path)\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "a = get_data(data_path + \"/table\", max_file_num=10)\n",
    "y = np.zeros((10,5))\n",
    "\n",
    "for index, _ in enumerate(y):\n",
    "    y[index][4] = 1\n",
    "print(y)\n",
    "    \n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        x_input = tf.placeholder(tf.float32, shape=[None, 32, 32, 32, 1], name=\"x_input\")\n",
    "        y_input = tf.placeholder(tf.float32, shape=[None, 5], name=\"y_input\") \n",
    "\n",
    "    prediction = cnn3d_model_with_default(params, x_input, 5)\n",
    "    tf.add_to_collection(\"logits\", prediction)\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_input), name=\"cross_entropy\")\n",
    "\n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "        tf.add_to_collection(\"optimizer\", optimizer)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'), name=\"acc\")\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    stop_point = tf.get_default_graph().get_tensor_by_name('conv5_layer/conv5:0')\n",
    "    \n",
    "    stopper = tf.stop_gradient(stop_point, 'stop_gradient')\n",
    "    \n",
    "    acc = sess.run([accuracy], feed_dict={x_input: a, y_input: y})\n",
    "    \n",
    "    print(acc)\n",
    "    \n",
    "     # Runs train_op.\n",
    "    _ , acc = sess.run([optimizer,accuracy], feed_dict={x_input: a, y_input: y})\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
