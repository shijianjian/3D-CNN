{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data comes from http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html\n",
    "from pyntcloud import PyntCloud\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if sys.platform == 'darwin':\n",
    "    data_path = os.getcwd() + \"/PartAnnotation\"\n",
    "else:\n",
    "    data_path = os.getcwd() + \"\\\\PartAnnotation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_var_names():\n",
    "    all_vars = []\n",
    "    for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "        all_vars.append(i.name)\n",
    "    return all_vars\n",
    "\n",
    "def get_all_placeholders():\n",
    "    return [x for x in tf.get_default_graph().get_operations() if x.type == \"Placeholder\"]\n",
    "\n",
    "def get_all_mean_op():\n",
    "    return [x for x in tf.get_default_graph().get_operations() if x.type == \"Mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/shijian/git/3D-CNN/3d_pointcloud/trained_model/model-4\n",
      "['conv1/kernel:0', 'conv1/bias:0', 'conv2/kernel:0', 'conv2/bias:0', 'conv4/kernel:0', 'conv4/bias:0', 'conv5/kernel:0', 'conv5/bias:0', 'conv7/kernel:0', 'conv7/bias:0', 'conv8/kernel:0', 'conv8/bias:0', 'bn/beta:0', 'bn/gamma:0', 'bn/moving_mean:0', 'bn/moving_variance:0', 'full_con/kernel:0', 'full_con/bias:0', 'y_pred/kernel:0', 'y_pred/bias:0', 'training/beta1_power:0', 'training/beta2_power:0', 'conv1/kernel/Adam:0', 'conv1/kernel/Adam_1:0', 'conv1/bias/Adam:0', 'conv1/bias/Adam_1:0', 'conv2/kernel/Adam:0', 'conv2/kernel/Adam_1:0', 'conv2/bias/Adam:0', 'conv2/bias/Adam_1:0', 'conv4/kernel/Adam:0', 'conv4/kernel/Adam_1:0', 'conv4/bias/Adam:0', 'conv4/bias/Adam_1:0', 'conv5/kernel/Adam:0', 'conv5/kernel/Adam_1:0', 'conv5/bias/Adam:0', 'conv5/bias/Adam_1:0', 'conv7/kernel/Adam:0', 'conv7/kernel/Adam_1:0', 'conv7/bias/Adam:0', 'conv7/bias/Adam_1:0', 'conv8/kernel/Adam:0', 'conv8/kernel/Adam_1:0', 'conv8/bias/Adam:0', 'conv8/bias/Adam_1:0', 'bn/beta/Adam:0', 'bn/beta/Adam_1:0', 'bn/gamma/Adam:0', 'bn/gamma/Adam_1:0', 'full_con/kernel/Adam:0', 'full_con/kernel/Adam_1:0', 'full_con/bias/Adam:0', 'full_con/bias/Adam_1:0', 'y_pred/kernel/Adam:0', 'y_pred/kernel/Adam_1:0', 'y_pred/bias/Adam:0', 'y_pred/bias/Adam_1:0']\n",
      "\n",
      "Optimizer [<tf.Operation 'training/Adam' type=NoOp>]\n",
      "\n",
      "Placeholders [<tf.Operation 'inputs/x_input' type=Placeholder>, <tf.Operation 'inputs/y_input' type=Placeholder>]\n",
      "\n",
      "Mean [<tf.Operation 'cross_entropy/batch_norm/bn/moments/Mean' type=Mean>, <tf.Operation 'cross_entropy/cost' type=Mean>, <tf.Operation 'acc' type=Mean>]\n"
     ]
    }
   ],
   "source": [
    "new_graph = tf.Graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True) # allow passing gpu-trained model to a cpu machine\n",
    "with tf.Session(graph=new_graph, config=config) as sess:\n",
    "\n",
    "    import os\n",
    "    model_fqn = os.path.join(os.getcwd(), 'trained_model', \"model-4\")\n",
    "    saver = tf.train.import_meta_graph(model_fqn + \".meta\")\n",
    "    saver.restore(sess, model_fqn)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    print(get_all_var_names())\n",
    "    print(\"\\nOptimizer\", graph.get_collection(\"optimizer\"))\n",
    "    print(\"\\nPlaceholders\", get_all_placeholders())\n",
    "    print(\"\\nMean\", get_all_mean_op())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelize3D(pts, dim=[1,1,1]):\n",
    "    \"\"\"\n",
    "    pts: receives .pts cloud point data. 2D array, arbitary sized X,Y,Z pairs. (We will only take x,y,z into account for now)\n",
    "    dim: dimensioin of output voxelized data\n",
    "    \n",
    "    This function will locate the grid cube and calculate the density of each cube.\n",
    "    The output will be normalized values.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1]>=3), \"pts file should contain at least x,y,z coordinate\"\n",
    "    assert(len(dim)==3), \"Please provide 3-d grid size like [32,32,32]\"\n",
    "    \n",
    "    # move all the axis to positive area.\n",
    "    minimum_val = [pts[0][0], pts[0][1], pts[0][2]]\n",
    "\n",
    "    # find the smallest \n",
    "    for pair in pts:\n",
    "        if pair[0] < minimum_val[0]:\n",
    "            minimum_val[0] = pair[0]\n",
    "        if pair[1] < minimum_val[1]:\n",
    "            minimum_val[1] = pair[1]\n",
    "        if pair[2] < minimum_val[2]:\n",
    "            minimum_val[2] = pair[2]\n",
    "            \n",
    "    # move it to first quadrant \n",
    "    rectified_pts = np.empty(pts.shape)\n",
    "    for index, pair in enumerate(pts):\n",
    "        point = np.zeros(3)\n",
    "        point[0] = pair[0] - minimum_val[0]\n",
    "        point[1] = pair[1] - minimum_val[1]\n",
    "        point[2] = pair[2] - minimum_val[2]\n",
    "        rectified_pts[index] = point\n",
    "    \n",
    "    # biggest value in each axis \n",
    "    maximum_val = pts[0][0]\n",
    "    \n",
    "    for pair in rectified_pts:\n",
    "        for val in pair:\n",
    "            if val > maximum_val:\n",
    "                maximum_val = val\n",
    "     \n",
    "    # normalize all the axises to (0,1)\n",
    "    normalized_pts = rectified_pts/maximum_val\n",
    "    \n",
    "    x_grid_length = 1/dim[0]\n",
    "    y_grid_length = 1/dim[1]\n",
    "    z_grid_length = 1/dim[2]\n",
    "    \n",
    "    output = np.zeros((dim[0],dim[1],dim[2]))\n",
    "    \n",
    "    epsilon = 0.000000000001 # we will have at least a 1.0 value which will exceed the index of grid\n",
    "    # we can use a relativly small value to escape that to fit our data\n",
    "    \n",
    "    max_volume_size = 0\n",
    "    \n",
    "    for pair in normalized_pts:\n",
    "        x_loc = int(pair[0]/(x_grid_length + epsilon))\n",
    "        y_loc = int(pair[1]/(y_grid_length + epsilon))\n",
    "        z_loc = int(pair[2]/(z_grid_length + epsilon))\n",
    "        if output[x_loc, y_loc, z_loc] is None:\n",
    "            output[x_loc, y_loc, z_loc] = 1\n",
    "        else:\n",
    "            output[x_loc, y_loc, z_loc] += 1\n",
    "        \n",
    "        if output[x_loc, y_loc, z_loc] > max_volume_size:\n",
    "            max_volume_size = output[x_loc, y_loc, z_loc]\n",
    "    \n",
    "    output = output/max_volume_size    \n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path, max_file_num=None, dim=[32,32,32]):\n",
    "    data = []\n",
    "    \n",
    "    target_dir_path = os.path.join(data_path, 'points')\n",
    "    path, dirs, files = os.walk(target_dir_path).__next__()\n",
    "    file_count = len(files)\n",
    "    \n",
    "    count = 0\n",
    "    for pts_data in os.scandir(target_dir_path):\n",
    "        if (max_file_num is None) or (count < max_file_num):\n",
    "            _path = os.path.join(data_path, 'points', pts_data.name)\n",
    "            pts = PyntCloud.from_file(_path, sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "            _vox = voxelize3D(pts.xyz, dim=dim)\n",
    "            vox_chan = np.array(_vox).reshape(_vox.shape + (1,))\n",
    "            data.append(vox_chan)\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(model_path):\n",
    "    \n",
    "    new_graph = tf.Graph()\n",
    "    config = tf.ConfigProto(allow_soft_placement=True) # allow passing gpu-trained model to a cpu machine\n",
    "    \n",
    "    with tf.Session(graph=new_graph, config=config) as sess:\n",
    "        \n",
    "        # Restore model\n",
    "        saver = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        model_graph = tf.get_default_graph()\n",
    "        \n",
    "        _conv1_w = model_graph.get_tensor_by_name('conv1/kernel:0')\n",
    "        _conv1_b = model_graph.get_tensor_by_name('conv1/bias:0')   \n",
    "\n",
    "        _conv2_w = model_graph.get_tensor_by_name('conv2/kernel:0')\n",
    "        _conv2_b = model_graph.get_tensor_by_name('conv2/bias:0')  \n",
    "\n",
    "        _conv4_w = model_graph.get_tensor_by_name('conv4/kernel:0')\n",
    "        _conv4_b = model_graph.get_tensor_by_name('conv4/bias:0')\n",
    "\n",
    "        _conv5_w = model_graph.get_tensor_by_name('conv5/kernel:0')\n",
    "        _conv5_b = model_graph.get_tensor_by_name('conv5/bias:0')\n",
    "        \n",
    "        _conv7_w = model_graph.get_tensor_by_name('conv7/kernel:0')\n",
    "        _conv7_b = model_graph.get_tensor_by_name('conv7/bias:0')\n",
    "        \n",
    "        _conv8_w = model_graph.get_tensor_by_name('conv8/kernel:0')\n",
    "        _conv8_b = model_graph.get_tensor_by_name('conv8/bias:0')\n",
    "    \n",
    "        _dict = {\n",
    "            'conv1_w': _conv1_w.eval(),\n",
    "            'conv1_b': _conv1_b.eval(),\n",
    "            'conv2_w': _conv2_w.eval(),\n",
    "            'conv2_b': _conv2_b.eval(),\n",
    "            'conv4_w': _conv4_w.eval(),\n",
    "            'conv4_b': _conv4_b.eval(),\n",
    "            'conv5_w': _conv5_w.eval(),\n",
    "            'conv5_b': _conv5_b.eval(),\n",
    "            'conv7_w': _conv7_w.eval(),\n",
    "            'conv7_b': _conv7_b.eval(),\n",
    "            'conv8_w': _conv8_w.eval(),\n",
    "            'conv8_b': _conv8_b.eval(),\n",
    "        }\n",
    "\n",
    "    return _dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3d(_input, params_w, params_b, stop_gradient=False, layer=None):\n",
    "    conv_w = tf.Variable(initial_value=params_w, name='w')\n",
    "    conv = tf.nn.conv3d(_input, conv_w, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv_b = tf.Variable(initial_value=params_b, name='b')\n",
    "    biaed_conv = tf.nn.bias_add(conv, conv_b, name=\"conv\" + str(layer))\n",
    "    \n",
    "    if stop_gradient:\n",
    "        biaed_conv = tf.stop_gradient(biaed_conv, 'stop_gradient' + str(layer))\n",
    "    \n",
    "    return tf.nn.relu(biaed_conv, name=\"relu\" + str(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn3d_model_with_default(params, x_input, output_label_size, stop_layer=0, keep_rate=0.5, seed=None):\n",
    "    \n",
    "    assert(stop_layer<=10)\n",
    "    assert(stop_layer>=0)\n",
    "    \n",
    "    if seed is not None:\n",
    "        tf.set_random_seed(seed)\n",
    "    \n",
    "    stop_gradients = np.zeros(10)\n",
    "    for i in range(len(stop_gradients)):\n",
    "        if stop_layer >= i:\n",
    "            stop_gradients[i] = True\n",
    "        else:\n",
    "            stop_gradients[i] = False\n",
    "    \n",
    "    with tf.name_scope('conv1_layer'):\n",
    "        conv1 = conv3d(x_input, params['conv1_w'], params['conv1_b'], stop_gradient=stop_gradients[1], layer=1)\n",
    "\n",
    "    with tf.name_scope('conv2_layer'):\n",
    "        conv2 = conv3d(conv1, params['conv2_w'], params['conv2_b'], stop_gradient=stop_gradients[2], layer=2)\n",
    "\n",
    "    with tf.name_scope('pool3_layer'):\n",
    "        pool3 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[2, 2, 2], strides=2, name=\"pool3\")\n",
    "\n",
    "    with tf.name_scope('conv4_layer'):\n",
    "        conv4 = conv3d(pool3, params['conv4_w'], params['conv4_b'], stop_gradient=stop_gradients[4], layer=4)\n",
    "\n",
    "    with tf.name_scope('conv5_layer'):\n",
    "        conv5 = conv3d(conv4, params['conv5_w'], params['conv5_b'], stop_gradient=stop_gradients[5], layer=5)\n",
    "        \n",
    "    with tf.name_scope('pool6_layer'):\n",
    "        pool6 = tf.layers.max_pooling3d(inputs=conv5, pool_size=[2, 2, 2], strides=2, name=\"pool6\")\n",
    "        \n",
    "    with tf.name_scope('conv7_layer'):\n",
    "        conv7 = conv3d(pool6, params['conv7_w'], params['conv7_b'], stop_gradient=stop_gradients[7], layer=7)\n",
    "        \n",
    "    with tf.name_scope('conv8_layer'):\n",
    "        conv8 = conv3d(conv7, params['conv8_w'], params['conv8_b'], stop_gradient=stop_gradients[8], layer=8)\n",
    "        \n",
    "    with tf.name_scope('pool9_layer'):\n",
    "        pool9 = tf.layers.max_pooling3d(inputs=conv8, pool_size=[2, 2, 2], strides=2, name=\"pool9\")\n",
    "           \n",
    "    with tf.name_scope(\"batch_norm\"):\n",
    "        cnn3d_bn = tf.layers.batch_normalization(inputs=pool9, training=False, name=\"bn\")\n",
    "        \n",
    "    with tf.name_scope(\"fully_con\"):\n",
    "        flattening = tf.reshape(cnn3d_bn, [-1, 4*4*4*512])\n",
    "        dense = tf.layers.dense(inputs=flattening, units=1024, activation=tf.nn.relu, name=\"full_con\")\n",
    "        # (1-keep_rate) is the probability that the node will be kept\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=keep_rate, training=False, name=\"dropout\")\n",
    "        \n",
    "    with tf.name_scope(\"y_conv\"):\n",
    "        y_conv = tf.layers.dense(inputs=dropout, units=output_label_size, name=\"y_pred\") \n",
    "        \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# one hot indexes\n",
    "\n",
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'trained_model', 'model-0')\n",
    "\n",
    "params = get_model_params(model_path)\n",
    "\n",
    "learning_rate = 40000000\n",
    "\n",
    "file_num =1000\n",
    "\n",
    "x_guitar = get_data(data_path + \"/guitar\", max_file_num=file_num) \n",
    "x_table = get_data(data_path + \"/table\", max_file_num=file_num)\n",
    "x_ = x_guitar + x_table\n",
    "y_ = np.zeros((len(x_),6))\n",
    "\n",
    "for index, _ in enumerate(y_):\n",
    "    if index < len(x_guitar):\n",
    "        y_[index][5] = 1\n",
    "    else:\n",
    "        y_[index][3] = 1\n",
    "        \n",
    "split_point = int(0.7*len(x_))\n",
    "x_train = x_[:split_point]\n",
    "y_train = y_[:split_point]\n",
    "\n",
    "x_test = x_[split_point:]\n",
    "y_test = y_[split_point:]\n",
    "    \n",
    "\n",
    "device_name = '/gpu:1' \n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    with tf.device(device_name):\n",
    "        with tf.name_scope('inputs'):\n",
    "            x_input = tf.placeholder(tf.float32, shape=[None, 32, 32, 32, 1], name=\"x_input\")\n",
    "            y_input = tf.placeholder(tf.float32, shape=[None, 6], name=\"y_input\") \n",
    "\n",
    "        prediction = cnn3d_model_with_default(params, x_input, 6, stop_layer=10, seed=1234)\n",
    "        tf.add_to_collection(\"logits\", prediction)\n",
    "\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_input), name=\"cross_entropy\")\n",
    "\n",
    "        with tf.name_scope(\"training\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "            tf.add_to_collection(\"optimizer\", optimizer)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'), name=\"acc\")\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    _conv1_w_prev = graph.get_tensor_by_name('conv1_layer/w:0').eval()\n",
    "    _conv1_b_prev = graph.get_tensor_by_name('conv1_layer/b:0').eval()\n",
    "    \n",
    "    _conv7_w_prev = graph.get_tensor_by_name('conv8_layer/w:0').eval()\n",
    "    _conv7_b_prev = graph.get_tensor_by_name('conv8_layer/b:0').eval()\n",
    "    \n",
    "    batch_size = 32\n",
    "    iterations_train = int(len(x_train)/batch_size) +1\n",
    "    iterations_test= int(len(x_test)/batch_size) +1\n",
    "    \n",
    "    acc = 0\n",
    "    \n",
    "    for itr in range(iterations_test):\n",
    "        mini_batch_x = x_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        mini_batch_y = y_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        _acc = sess.run(accuracy, feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "        acc += _acc\n",
    "        print('\\tLost for', itr+1, \"/\", iterations_test, _acc, end='\\r')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Test Acc:', acc/iterations_test)\n",
    "    \n",
    "    for itr in range(iterations_train):\n",
    "        mini_batch_x = x_train[itr*batch_size: (itr+1)*batch_size]\n",
    "        mini_batch_y = y_train[itr*batch_size: (itr+1)*batch_size]\n",
    "        _optimizer, _cost = sess.run([optimizer, cost], feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "        print('\\tLost for', itr+1, \"/\", iterations_train, _cost, end='\\r')\n",
    "    \n",
    "    print('\\n')\n",
    "        \n",
    "    _conv1_w_aft = graph.get_tensor_by_name('conv1_layer/w:0').eval()\n",
    "    _conv1_b_aft = graph.get_tensor_by_name('conv1_layer/b:0').eval()\n",
    "    \n",
    "    _conv7_w_aft = graph.get_tensor_by_name('conv8_layer/w:0').eval()\n",
    "    _conv7_b_aft = graph.get_tensor_by_name('conv8_layer/b:0').eval()\n",
    "    \n",
    "    print((_conv1_w_prev == _conv1_w_aft).all())\n",
    "    print((_conv1_b_prev == _conv1_b_aft).all())\n",
    "    \n",
    "    print((_conv7_w_prev == _conv7_w_aft).all())\n",
    "    print((_conv7_b_prev == _conv7_b_aft).all())\n",
    "    \n",
    "    acc = 0\n",
    "    for itr in range(iterations_test):\n",
    "        mini_batch_x = x_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        mini_batch_y = y_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        _acc = sess.run(accuracy, feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "        acc += _acc\n",
    "        print('\\tLost for', itr+1, \"/\", iterations_test, cost, end='\\r')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Test Acc:', acc/iterations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File C:\\Users\\212606295\\Desktop\\3D-CNN\\3d_pointcloud\\trained_model\\model-0.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-317074141b34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trained_model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model-0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfile_num\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-a1cd4767ea9a>\u001b[0m in \u001b[0;36mget_model_params\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Restore model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".meta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[0;32m   1801\u001b[0m                      \"execution is enabled.\")\n\u001b[0;32m   1802\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1803\u001b[1;33m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1804\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1805\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    553\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File %s does not exist.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m   \u001b[1;31m# First try to read it as a binary file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: File C:\\Users\\212606295\\Desktop\\3D-CNN\\3d_pointcloud\\trained_model\\model-0.meta does not exist."
     ]
    }
   ],
   "source": [
    "# one hot indexes\n",
    "# 2: chair , 3: car\n",
    "\n",
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'trained_model', 'model-0')\n",
    "\n",
    "params = get_model_params(model_path)\n",
    "\n",
    "file_num =7000\n",
    "\n",
    "a = get_data(data_path + \"/airplane\", max_file_num=file_num)\n",
    "    \n",
    "device_name = '/gpu:1' \n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    with tf.device(device_name):\n",
    "        with tf.name_scope('inputs'):\n",
    "            x_input = tf.placeholder(tf.float32, shape=[None, 32, 32, 32, 1], name=\"x_input\")\n",
    "            y_input = tf.placeholder(tf.float32, shape=[None, 5], name=\"y_input\") \n",
    "\n",
    "        prediction = cnn3d_model_with_default(params, x_input, 5, stop_layer=10, seed=1234)\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'), name=\"acc\")\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    batch_size = 32\n",
    "    accu = []\n",
    "    iterations_test = int(file_num/batch_size) +1\n",
    "    for itr in range(iterations_test):\n",
    "        mini_batch_x = a[itr*batch_size: (itr+1)*batch_size]\n",
    "        mini_batch_y = y[itr*batch_size: (itr+1)*batch_size]\n",
    "#         acc = sess.run(accuracy, feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "        if len(mini_batch_x) == 0:\n",
    "            continue\n",
    "        p = np.argmax(sess.run(prediction, feed_dict={x_input: mini_batch_x}), 1)\n",
    "        accu.append(p)\n",
    "        print('\\taccuracy for', itr+1, \"/\", iterations_train, p, end='\\r')\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    pppp = {}\n",
    "    for _ in range(len(accu)):\n",
    "        for i,v in enumerate(accu[_]):\n",
    "            if accu[_][i] in pppp:\n",
    "                pppp[v] = pppp[v] + 1\n",
    "            else:\n",
    "                pppp[v] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1343, 1: 476, 2: 139, 3: 2684, 4: 2346, 5: 12}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./voxel_grid_plot.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_voxelgrid(voxelgrid,\n",
    "                   output_name=None,\n",
    "                   cmap=\"Oranges\",\n",
    "                   axis=True,\n",
    "                   width=800,\n",
    "                   height=600):\n",
    "\n",
    "    scaled_shape = voxelgrid.shape\n",
    "\n",
    "    vector = voxelgrid\n",
    "    points = np.argwhere(vector) * scaled_shape\n",
    "\n",
    "    s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    rgb = s_m.to_rgba(vector.reshape(-1)[vector.reshape(-1) > 0])\n",
    "\n",
    "    camera_position = points.max(0) + abs(points.max(0))\n",
    "\n",
    "    look = points.mean(0)\n",
    "\n",
    "    if axis:\n",
    "        axis_size = points.ptp() * 1.5\n",
    "    else:\n",
    "        axis_size = 0\n",
    "\n",
    "    placeholders = {}\n",
    "\n",
    "    placeholders[\"POINTS_X_PLACEHOLDER\"] = points[:, 0].tolist()\n",
    "    placeholders[\"POINTS_Y_PLACEHOLDER\"] = points[:, 1].tolist()\n",
    "    placeholders[\"POINTS_Z_PLACEHOLDER\"] = points[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"R_PLACEHOLDER\"] = rgb[:, 0].tolist()\n",
    "    placeholders[\"G_PLACEHOLDER\"] = rgb[:, 1].tolist()\n",
    "    placeholders[\"B_PLACEHOLDER\"] = rgb[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"S_x_PLACEHOLDER\"] = scaled_shape[0]\n",
    "    placeholders[\"S_y_PLACEHOLDER\"] = scaled_shape[1]\n",
    "    placeholders[\"S_z_PLACEHOLDER\"] = scaled_shape[2]\n",
    "\n",
    "    placeholders[\"CAMERA_X_PLACEHOLDER\"] = camera_position[0]\n",
    "    placeholders[\"CAMERA_Y_PLACEHOLDER\"] = camera_position[1]\n",
    "    placeholders[\"CAMERA_Z_PLACEHOLDER\"] = camera_position[2]\n",
    "\n",
    "    placeholders[\"LOOK_X_PLACEHOLDER\"] = look[0]\n",
    "    placeholders[\"LOOK_Y_PLACEHOLDER\"] = look[1]\n",
    "    placeholders[\"LOOK_Z_PLACEHOLDER\"] = look[2]\n",
    "\n",
    "    placeholders[\"AXIS_SIZE_PLACEHOLDER\"] = axis_size\n",
    "\n",
    "    placeholders[\"N_VOXELS_PLACEHOLDER\"] = sum(vector.reshape(-1) > 0)\n",
    "\n",
    "    if output_name is None:\n",
    "        output_name = \"plotVG.html\"\n",
    "\n",
    "    BASE_PATH = os.getcwd()\n",
    "    src = \"{}/{}\".format(BASE_PATH, \"voxelgrid.html\")\n",
    "    dst = \"{}/{}\".format(os.getcwd(), output_name)\n",
    "\n",
    "    with open(src, \"r\") as inp, open(dst, \"w\") as out:\n",
    "        for line in inp:\n",
    "            for key, val in placeholders.items():\n",
    "                if key in line:\n",
    "                    line = line.replace(key, str(val))\n",
    "            out.write(line)\n",
    "\n",
    "    return IFrame(output_name, width=width, height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/shijian/git/3D-CNN/3d_pointcloud/trained_model/model-4\n",
      "(3, 3, 3, 1, 16)\n"
     ]
    }
   ],
   "source": [
    "# one hot indexes\n",
    "# 2: chair , 3: car\n",
    "\n",
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'trained_model', 'model-4')\n",
    "\n",
    "params = get_model_params(model_path)\n",
    "\n",
    "print(params['conv1_w'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num =7000\n",
    "\n",
    "a = get_data(data_path + \"/airplane\", max_file_num=file_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][:3, :3, :3, :1] * params['conv1_w'][:,:,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3, 1, 16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['conv1_w'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_padding(target, kernel, stride, padding='SAME'):\n",
    "    \n",
    "    size, kernelsize, color_channels = get_shapes(target, kernel)\n",
    "    \n",
    "    import math\n",
    "    out_weight = math.ceil(float(size) / float(stride))\n",
    "    if padding is 'SAME':\n",
    "        pad_along_width = max((out_weight - 1) * stride + kernelsize - size, 0)\n",
    "        pad_left_and_top = pad_along_width // 2\n",
    "        pad_right_and_bottom = pad_along_width - pad_left_and_top\n",
    "        padded_target = zero_padding_3d(target, (int((pad_left_and_top)), int(pad_right_and_bottom)))\n",
    "        print(\"Padded shape\", padded_target.shape)\n",
    "    elif padding is 'VALID':\n",
    "        raise TypeError('Not implemeneted')\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError('Padding strategy `' + padding + '` not found.')\n",
    "    \n",
    "    return padded_target\n",
    "\n",
    "\n",
    "def zero_padding_3d(target, padding=None):\n",
    "    if padding is not None:\n",
    "        target = np.pad(target, [(padding[0], padding[1]), (padding[0], padding[1]), (padding[0], padding[1]), (0,0)], mode='constant')\n",
    "    return target\n",
    "\n",
    "\n",
    "def single_convolve_3d(padded_target_slice, kernel):\n",
    "    _slice = padded_target_slice * kernel\n",
    "    res = np.sum(np.sum(np.sum(_slice, axis=0), axis=0),axis=0)\n",
    "    return res\n",
    "\n",
    "\n",
    "def convole_3d(padded_target, kernel, stride):\n",
    "    \n",
    "    size, kernelsize, color_channels = get_shapes(padded_target, kernel)\n",
    "    outsize = get_outsize(size, kernelsize, stride)\n",
    "    filter_num = num_of_filters(kernel)\n",
    "    \n",
    "    res = np.zeros((outsize, outsize, outsize, color_channels, filter_num))\n",
    "    \n",
    "    for i in range(outsize):\n",
    "        for j in range(outsize):\n",
    "            for k in range(outsize):\n",
    "                for c in range(color_channels):\n",
    "                    for f in range(filter_num):\n",
    "                        i_start = i * stride\n",
    "                        j_start = j * stride\n",
    "                        k_start = k * stride\n",
    "\n",
    "                        padded_target_slice = padded_target[i_start : i_start+kernelsize, j_start : j_start+kernelsize, k_start : k_start+kernelsize, c]\n",
    "                        res[i_start, j_start, k_start, c, f] = single_convolve_3d(padded_target_slice, kernel[:,:,:,c,f])\n",
    "                    \n",
    "    print(res.shape)\n",
    "    return res\n",
    "   \n",
    "    \n",
    "def get_shapes(target, kernel):\n",
    "    size = target.shape[0]\n",
    "    color_channels = target.shape[3]\n",
    "    kernelsize = kernel.shape[0]\n",
    "    assert(color_channels == kernel.shape[3]), \"The shape of kernel and input does not match, it must have same number of channels.\"\n",
    "    return size, kernelsize, color_channels\n",
    "\n",
    "def get_outsize(size, kernelsize, stride):\n",
    "    outsize = (size-kernelsize)/stride + 1\n",
    "    assert(outsize == int(outsize)), \"Number of filters must be integer\"\n",
    "    \n",
    "    return int(outsize)\n",
    "\n",
    "def num_of_filters(kernel):\n",
    "    return kernel.shape[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_features(target, kernel, stride=1, padding='SAME'):\n",
    "    \n",
    "    size, kernelsize, color_channels = get_shapes(target, kernel)\n",
    "    outsize = get_outsize(size, kernelsize, stride)\n",
    "    filter_num = num_of_filters(kernel)\n",
    "    \n",
    "    padded_target = do_padding(target, kernel, stride)\n",
    "    output = convole_3d(padded_target, kernel, stride)\n",
    "    \n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape (34, 34, 34, 1)\n",
      "(32, 32, 32, 1, 16)\n",
      "(32, 32, 32, 1, 16)\n"
     ]
    }
   ],
   "source": [
    "hahaha = output_features(a[0], params['conv1_w'], stride=1, padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_voxelgrid() missing 1 required positional argument: 'voxelgrid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-271-a85575fee61c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_voxelgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: plot_voxelgrid() missing 1 required positional argument: 'voxelgrid'"
     ]
    }
   ],
   "source": [
    "plot_voxelgrid(hahaha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
