{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data comes from http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html\n",
    "from pyntcloud import PyntCloud\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if sys.platform == 'darwin':\n",
    "    data_path = os.getcwd() + \"/PartAnnotation\"\n",
    "else:\n",
    "    data_path = os.getcwd() + \"\\\\PartAnnotation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes from http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html\n",
    "\n",
    "Rename the folder into proper class names if you want to see the results properly. It won't affect the results but it is better to see a proper label rather than random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_data(data_path):\n",
    "    \"\"\"\n",
    "    Will read all the data under specific data path\n",
    "    \n",
    "    Return:\n",
    "        cate: a dict which all the names encoded by numbers\n",
    "        label: a dict contains all the number & name mappings.\n",
    "    \"\"\"\n",
    "    cate = {}\n",
    "    label = {}\n",
    "    count = 0\n",
    "    for entry in os.scandir(data_path):\n",
    "        if entry.is_dir():\n",
    "            for pts_data in os.scandir(os.path.join(data_path, entry.name, 'points')):\n",
    "                if count in cate:\n",
    "                    cate.update({count: cate.get(count) + 1})\n",
    "                else:\n",
    "                    cate.update({count: 1})\n",
    "                    label.update({count: entry.name})\n",
    "            count += 1\n",
    "    return cate, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cate, label = count_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 73, 1: 2318, 2: 7497, 3: 307, 4: 797, 5: 6778, 6: 337, 7: 56, 8: 8509, 9: 4045, 10: 152, 11: 214, 12: 85, 13: 460, 14: 83, 15: 424}\n",
      "{0: 'earphone', 1: 'lamp', 2: 'car', 3: 'pistol', 4: 'guitar', 5: 'chair', 6: 'motorbike', 7: 'cap', 8: 'table', 9: 'airplane', 10: 'skateboard', 11: 'mug', 12: 'rocket', 13: 'laptop', 14: 'bag', 15: 'knife'}\n"
     ]
    }
   ],
   "source": [
    "print(cate)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAGfCAYAAAD8uyvQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngAkQ7gAAACZAuAMAAJgA4Q4AAGAC/n+PdA7p7BqcVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e162978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.bar(left=list(cate), height=cate.values())\n",
    "plt.xticks(list(cate), list(label.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Data Imbalance problem\n",
    "\n",
    "Method 1: pick same amount of samples from each class\n",
    "\n",
    "Method 2: Repeat small sample classes 10 times or so.\n",
    "\n",
    "    - Add Gaussian Noise\n",
    "    - Add invariant transformations(shift left, right, rotate, etc.)\n",
    "\n",
    "Method 2 is more preferable. since less variance, more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data(data_path, max_file_num=None, min_file_num=None):\n",
    "    \"\"\"\n",
    "    Find file in each folder according to the 'data_path'.\n",
    "    \n",
    "    Giving the max and min number of files via `max_file_num`, `min_file_num`.\n",
    "    \n",
    "    `min_file_num` will skip the folders which doesn't meet the `min_file_num` threshold.\n",
    "    \"\"\"\n",
    "    if max_file_num is not None and min_file_num is not None:\n",
    "        assert(max_file_num > min_file_num), \"`max_file_num` should be greater than `min_file_num`\"\n",
    "        \n",
    "    data = []\n",
    "    label = []\n",
    "    for entry in os.scandir(data_path):\n",
    "        if entry.is_dir():\n",
    "            target_dir_path = os.path.join(data_path, entry.name, 'points')\n",
    "            path, dirs, files = os.walk(target_dir_path).__next__()\n",
    "            file_count = len(files)\n",
    "            if (min_file_num is None) or (file_count > min_file_num):\n",
    "                count = 0\n",
    "                for pts_data in os.scandir(target_dir_path):\n",
    "                    if (max_file_num is None) or (count < max_file_num):\n",
    "                        data.append(os.path.join(data_path, entry.name, 'points', pts_data.name))\n",
    "                        label.append(entry.name)\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        break\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = find_data(data_path, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_point_cloud = PyntCloud.from_file(data[30], sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pistol'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"pyntcloud_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x162c97160>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_point_cloud.add_structure(\"voxelgrid\", x_y_z=[32, 32, 32])\n",
    "\n",
    "my_point_cloud.plot(point_size=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def voxelize3D(pts, dim=[1,1,1]):\n",
    "    \"\"\"\n",
    "    pts: receives .pts cloud point data. 2D array, arbitary sized X,Y,Z pairs.\n",
    "    dim: dimensioin of output voxelized data\n",
    "    \n",
    "    .pts data X, Y, Z ranged from -0.5~0.5.\n",
    "    This function will locate the grid cube and calculate the density of each cube.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1]==3)\n",
    "    assert(len(dim)==3)\n",
    "    \n",
    "    x_grid_length = 1/dim[0]\n",
    "    y_grid_length = 1/dim[1]\n",
    "    z_grid_length = 1/dim[2]\n",
    "    \n",
    "    output = np.zeros((dim[0],dim[1],dim[2])) # Make a 10 by 20 by 30 array\n",
    "    \n",
    "    for pair in pts:\n",
    "        x_loc = int(pair[0]/x_grid_length) + int(dim[0]/2)\n",
    "        y_loc = int(pair[1]/y_grid_length) + int(dim[1]/2)\n",
    "        z_loc = int(pair[2]/z_grid_length) + int(dim[2]/2)\n",
    "        if output[x_loc, y_loc, z_loc] is None:\n",
    "            output[x_loc, y_loc, z_loc] = 1\n",
    "        else:\n",
    "            output[x_loc, y_loc, z_loc] += 1\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ./voxel_grid_plot.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_voxelgrid(voxelgrid,\n",
    "                   output_name=None,\n",
    "                   cmap=\"Oranges\",\n",
    "                   axis=True,\n",
    "                   width=800,\n",
    "                   height=600):\n",
    "\n",
    "    scaled_shape = voxelgrid.shape\n",
    "\n",
    "    vector = voxelgrid\n",
    "    points = np.argwhere(vector) * scaled_shape\n",
    "\n",
    "    s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    rgb = s_m.to_rgba(vector.reshape(-1)[vector.reshape(-1) > 0])\n",
    "\n",
    "    camera_position = points.max(0) + abs(points.max(0))\n",
    "\n",
    "    look = points.mean(0)\n",
    "\n",
    "    if axis:\n",
    "        axis_size = points.ptp() * 1.5\n",
    "    else:\n",
    "        axis_size = 0\n",
    "\n",
    "    placeholders = {}\n",
    "\n",
    "    placeholders[\"POINTS_X_PLACEHOLDER\"] = points[:, 0].tolist()\n",
    "    placeholders[\"POINTS_Y_PLACEHOLDER\"] = points[:, 1].tolist()\n",
    "    placeholders[\"POINTS_Z_PLACEHOLDER\"] = points[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"R_PLACEHOLDER\"] = rgb[:, 0].tolist()\n",
    "    placeholders[\"G_PLACEHOLDER\"] = rgb[:, 1].tolist()\n",
    "    placeholders[\"B_PLACEHOLDER\"] = rgb[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"S_x_PLACEHOLDER\"] = scaled_shape[0]\n",
    "    placeholders[\"S_y_PLACEHOLDER\"] = scaled_shape[1]\n",
    "    placeholders[\"S_z_PLACEHOLDER\"] = scaled_shape[2]\n",
    "\n",
    "    placeholders[\"CAMERA_X_PLACEHOLDER\"] = camera_position[0]\n",
    "    placeholders[\"CAMERA_Y_PLACEHOLDER\"] = camera_position[1]\n",
    "    placeholders[\"CAMERA_Z_PLACEHOLDER\"] = camera_position[2]\n",
    "\n",
    "    placeholders[\"LOOK_X_PLACEHOLDER\"] = look[0]\n",
    "    placeholders[\"LOOK_Y_PLACEHOLDER\"] = look[1]\n",
    "    placeholders[\"LOOK_Z_PLACEHOLDER\"] = look[2]\n",
    "\n",
    "    placeholders[\"AXIS_SIZE_PLACEHOLDER\"] = axis_size\n",
    "\n",
    "    placeholders[\"N_VOXELS_PLACEHOLDER\"] = sum(vector.reshape(-1) > 0)\n",
    "\n",
    "    if output_name is None:\n",
    "        output_name = \"plotVG.html\"\n",
    "\n",
    "    BASE_PATH = os.getcwd()\n",
    "    src = \"{}/{}\".format(BASE_PATH, \"voxelgrid.html\")\n",
    "    dst = \"{}/{}\".format(os.getcwd(), output_name)\n",
    "\n",
    "    with open(src, \"r\") as inp, open(dst, \"w\") as out:\n",
    "        for line in inp:\n",
    "            for key, val in placeholders.items():\n",
    "                if key in line:\n",
    "                    line = line.replace(key, str(val))\n",
    "            out.write(line)\n",
    "\n",
    "    return IFrame(output_name, width=width, height=height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoom in a little bit, seems like some visual bugs inside the code if you have a large set of points\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x162c8c898>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Zoom in a little bit, seems like some visual bugs inside the code if you have a large set of points\")\n",
    "vox = voxelize3D(my_point_cloud.xyz, dim=[32,32,32])\n",
    "plot_voxelgrid(vox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_cates(labels):\n",
    "    cates = []\n",
    "    for l in labels:\n",
    "        if l not in cates:\n",
    "            cates.append(l)\n",
    "    return cates\n",
    "\n",
    "def data_onehot_encode(labels):\n",
    "    \"\"\"\n",
    "    Recieves an array of labels.\n",
    "    \"\"\"\n",
    "    cates = label_cates(labels)\n",
    "    # one-hot\n",
    "    onehot = []\n",
    "    for l in labels:\n",
    "        x = np.zeros(len(cates))\n",
    "        x[cates.index(l)] = 1.0\n",
    "        onehot.append(x)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_reshape(data):\n",
    "    \"\"\"\n",
    "    Will read and voxelize the data\n",
    "    \"\"\"\n",
    "    x_reshaped = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if i % 20 == 0:\n",
    "            print(\"Process: \", str('{0:.2f}'.format(i/len(data))) + \"%\", end='\\r')\n",
    "        my_point_cloud = PyntCloud.from_file(data[i], sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "        vox = voxelize3D(my_point_cloud.xyz, [32,32,32])\n",
    "        vox_chan = np.array(vox).reshape(vox.shape + (1,))\n",
    "        x_reshaped.append(vox_chan)\n",
    "        \n",
    "    print(\"Process: \", \" 100% \")        \n",
    "    return x_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_swap(data, index_a, index_b):\n",
    "    temp = data[index_a]\n",
    "    data[index_a] = data[index_b]\n",
    "    data[index_b] = temp\n",
    "    return data\n",
    "\n",
    "def data_random_position(data):\n",
    "    import random\n",
    "    return random.randint(0,len(data)-1)\n",
    "\n",
    "def data_shuffling(data, label):\n",
    "    for i in range(len(data)):\n",
    "        target = data_random_position(data)\n",
    "        data = data_swap(data, i, target)\n",
    "        label = data_swap(label, i, target)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data_raw, shuffled_label_raw = data_shuffling(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process:   100% \n"
     ]
    }
   ],
   "source": [
    "shuffled_data = data_reshape(shuffled_data_raw)\n",
    "shuffled_label = data_onehot_encode(shuffled_label_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_classes = label_cates(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 1)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "['table', 'chair', 'lamp', 'knife', 'car', 'skateboard', 'rocket', 'guitar', 'airplane', 'bag', 'pistol', 'laptop', 'motorbike', 'cap', 'earphone', 'mug']\n"
     ]
    }
   ],
   "source": [
    "print(shuffled_data[100].shape)\n",
    "print(shuffled_label[100])\n",
    "print(label_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "Data normalization for different size of pics.\n",
    "\n",
    "It does not matter for this dataset since we got similar size of pics. But if we have a pic with 10MB, the density of the pic would be larger than what we have now.\n",
    "\n",
    "The point is to normalize all the data to standard level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    x_input = tf.placeholder(tf.float32, shape=[None, 32, 32, 32, 1])\n",
    "    y_input = tf.placeholder(tf.float32, shape=[None, len(label_classes)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(x_train_data, label_size, keep_rate=0.7, seed=None):\n",
    "    \n",
    "    with tf.name_scope(\"layer_a\"):\n",
    "        # conv => 32*32*32\n",
    "        conv1 = tf.layers.conv3d(inputs=x_train_data, filters=16, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # conv => 32*32*32\n",
    "        conv2 = tf.layers.conv3d(inputs=conv1, filters=32, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # pool => 16*16*16\n",
    "        pool3 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[2, 2, 2], strides=2)\n",
    "        \n",
    "    with tf.name_scope(\"layer_b\"):\n",
    "        # conv => 16*16*16\n",
    "        conv4 = tf.layers.conv3d(inputs=pool3, filters=64, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # conv => 16*16*16\n",
    "        conv5 = tf.layers.conv3d(inputs=conv4, filters=128, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # pool => 8*8*8\n",
    "        pool6 = tf.layers.max_pooling3d(inputs=conv5, pool_size=[2, 2, 2], strides=2)\n",
    "        \n",
    "    with tf.name_scope(\"layer_c\"):\n",
    "        # conv => 8*8*8\n",
    "        conv7 = tf.layers.conv3d(inputs=pool6, filters=256, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # conv => 8*8*8\n",
    "        conv8 = tf.layers.conv3d(inputs=conv7, filters=512, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # pool => 4*4*4\n",
    "        pool9 = tf.layers.max_pooling3d(inputs=conv8, pool_size=[2, 2, 2], strides=2)\n",
    "        \n",
    "    with tf.name_scope(\"batch_norm\"):\n",
    "        cnn3d_bn = tf.layers.batch_normalization(inputs=pool9, training=True)\n",
    "        \n",
    "    with tf.name_scope(\"fully_con\"):\n",
    "        flattening = tf.reshape(cnn3d_bn, [-1, 4*4*4*512])\n",
    "        dense = tf.layers.dense(inputs=flattening, units=1024, activation=tf.nn.relu)\n",
    "        # (1-keep_rate) is the probability that the node will be kept\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=keep_rate, training=True)\n",
    "        \n",
    "    with tf.name_scope(\"y_conv\"):\n",
    "        y_conv = tf.layers.dense(inputs=dropout, units=label_size)\n",
    "    \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(x_train_data, y_train_data, x_test_data, y_test_data, learning_rate=0.1, keep_rate=0.7, epochs=10, batch_size=128, using_gpu=False):\n",
    "\n",
    "    if using_gpu:\n",
    "        device_name = '/gpu:1' \n",
    "    else:\n",
    "        device_name = '/cpu:0'\n",
    "\n",
    "    with tf.device(device_name):\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            prediction = cnn_model(x_input, len(cate), keep_rate, seed=1)\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_input))\n",
    "                              \n",
    "        with tf.name_scope(\"training\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        \n",
    "    sess = tf.InteractiveSession()\n",
    "   \n",
    "    if using_gpu:\n",
    "        # GPU using BFC\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allocator_type = 'BFC'\n",
    "        sess =  tf.Session(config= config)\n",
    "            \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    import datetime\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    iterations = int(len(x_train_data)/batch_size) + 1\n",
    "    # run epochs\n",
    "    for epoch in range(epochs):\n",
    "        start_time_epoch = datetime.datetime.now()\n",
    "        print('Epoch', epoch, 'started')\n",
    "        \n",
    "        # mini batch\n",
    "        for itr in range(iterations):\n",
    "            mini_batch_x = x_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "            mini_batch_y = y_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "            _optimizer, _cost = sess.run([optimizer, cost], feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "            print('\\tLost', _cost, end='\\r')\n",
    "\n",
    "        acc = sess.run(accuracy, feed_dict={x_input: x_test_data, y_input: y_test_data})\n",
    "        \n",
    "        end_time_epoch = datetime.datetime.now()\n",
    "        print('Testing Set Accuracy:',acc, ' Time elapse: ', str(end_time_epoch - start_time_epoch))\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    print('Time elapse: ', str(end_time - start_time))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cross_validation:\n",
    "    \n",
    "    def __init__(self, data, label, fold=5):\n",
    "        self.data = np.asarray(data)\n",
    "        self.label = np.asarray(label)\n",
    "        assert(len(self.data) == len(self.label))\n",
    "        self.fold = fold\n",
    "        self.current_itr = 0;\n",
    "        self.sample_in_fold = int(len(data)/fold)\n",
    "\n",
    "    def next_bunch(self):\n",
    "        \"\"\"\n",
    "        Return x_train, y_train, x_test, y_test\n",
    "        \"\"\"\n",
    "        assert(self.current_itr < self.fold)\n",
    "        start = self.current_itr * self.sample_in_fold\n",
    "        end = (self.current_itr + 1) * self.sample_in_fold\n",
    "        self.current_itr += 1\n",
    "        if start == 0:\n",
    "            return self.data[end:], self.label[end:], self.data[:end], self.label[:end]\n",
    "        if end >= len(self.data):\n",
    "            return self.data[:start], self.label[:start], self.data[start:], self.label[start:]\n",
    "\n",
    "        return np.concatenate((self.data[:start],self.data[end:])), np.concatenate((self.label[:start],self.label[end:])), self.data[start : end], self.label[start : end]\n",
    "\n",
    "    def have_next(self):\n",
    "        if self.current_itr != self.fold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n",
      "Testing Set Accuracy: 0.0703125  Time elapse:  0:23:14.522351\n",
      "Epoch 1 started\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-622100ec644f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhave_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0m_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_bunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musing_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-05beb7409ca4>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(x_train_data, y_train_data, x_test_data, y_test_data, learning_rate, keep_rate, epochs, batch_size, using_gpu)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mmini_batch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mmini_batch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmini_batch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmini_batch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tLost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = cross_validation(shuffled_data, shuffled_label, fold=5)\n",
    "\n",
    "while cv.have_next():\n",
    "    _x_train, _y_train, _x_test, _y_test = cv.next_bunch()\n",
    "    train_neural_network(_x_train[:], _y_train[:], _x_train[:], _y_train[:], learning_rate=0.01, batch_size=16, epochs=5, using_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
