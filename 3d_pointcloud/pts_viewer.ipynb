{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data comes from http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html\n",
    "from pyntcloud import PyntCloud\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if sys.platform == 'darwin':\n",
    "    data_path = os.getcwd() + \"/PartAnnotation\"\n",
    "else:\n",
    "    data_path = os.getcwd() + \"\\\\PartAnnotation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes from http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html\n",
    "\n",
    "Rename the folder into proper class names if you want to see the results properly. It won't affect the results but it is better to see a proper label rather than random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_data(data_path):\n",
    "    \"\"\"\n",
    "    Will read all the data under specific data path\n",
    "    \n",
    "    Return:\n",
    "        cate: a dict which all the names encoded by numbers\n",
    "        label: a dict contains all the number & name mappings.\n",
    "    \"\"\"\n",
    "    cate = {}\n",
    "    label = {}\n",
    "    count = 0\n",
    "    for entry in os.scandir(data_path):\n",
    "        if entry.is_dir():\n",
    "            for pts_data in os.scandir(os.path.join(data_path, entry.name, 'points')):\n",
    "                if count in cate:\n",
    "                    cate.update({count: cate.get(count) + 1})\n",
    "                else:\n",
    "                    cate.update({count: 1})\n",
    "                    label.update({count: entry.name})\n",
    "            count += 1\n",
    "    return cate, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cate, label = count_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 73, 1: 2318, 2: 7497, 3: 307, 4: 797, 5: 6778, 6: 337, 7: 56, 8: 8509, 9: 4045, 10: 152, 11: 214, 12: 85, 13: 460, 14: 83, 15: 424}\n",
      "{0: 'earphone', 1: 'lamp', 2: 'car', 3: 'pistol', 4: 'guitar', 5: 'chair', 6: 'motorbike', 7: 'cap', 8: 'table', 9: 'airplane', 10: 'skateboard', 11: 'mug', 12: 'rocket', 13: 'laptop', 14: 'bag', 15: 'knife'}\n"
     ]
    }
   ],
   "source": [
    "print(cate)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAGfCAYAAAD8uyvQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wNaAyEZCvumbQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e163198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.bar(list(cate), height=list(cate.values()))\n",
    "plt.xticks(list(cate), list(label.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imbalance problem\n",
    "\n",
    "Method 1: pick same amount of samples from each class\n",
    "\n",
    "Method 2: Repeat small sample classes 10 times or so.\n",
    "\n",
    "    - Add Gaussian Noise\n",
    "    - Add invariant transformations(shift left, right, rotate, etc.)\n",
    "\n",
    "Method 2 is more preferable. since less variance, more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_data(data_path, max_file_num=None, folder_filter=(None, None)):\n",
    "    \"\"\"\n",
    "    Find file in each folder according to the 'data_path'.\n",
    "    \n",
    "    Giving the max number of files via `max_file_num`, it will read first `max_file_num` in each folder. Read all if there is no enough file inside.\n",
    "    \n",
    "    `folder_filter` is a tuple like (100, 2000) which indicates the number of files will between 100 and 2000.\n",
    "    \"\"\"\n",
    "    \n",
    "    min_th, max_th = folder_filter\n",
    "    if max_file_num is not None and folder_filter is not None:\n",
    "        if min_th is not None:\n",
    "            assert(max_file_num > min_th), \"`max_file_num` should be greater than `\" + min_th + \"` in \" + folder_filter\n",
    "        \n",
    "    data = []\n",
    "    label = []\n",
    "    for entry in os.scandir(data_path):\n",
    "        if entry.is_dir():\n",
    "            target_dir_path = os.path.join(data_path, entry.name, 'points')\n",
    "            path, dirs, files = os.walk(target_dir_path).__next__()\n",
    "            file_count = len(files)\n",
    "            if (min_th is None or file_count >= min_th) and (max_th is None or file_count <= max_th):\n",
    "                count = 0\n",
    "                for pts_data in os.scandir(target_dir_path):\n",
    "                    if (max_file_num is None) or (count < max_file_num):\n",
    "                        data.append(os.path.join(data_path, entry.name, 'points', pts_data.name))\n",
    "                        label.append(entry.name)\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        break\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, label = find_data(data_path, folder_filter=(10, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_point_cloud = PyntCloud.from_file(data[30], sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earphone'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"pyntcloud_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11e2158d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_point_cloud.plot(point_size=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "- Rotate\n",
    "- Squeeze\n",
    "- Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rotate(pts, axis=0, angle_degree=30):\n",
    "    \"\"\"\n",
    "    Rotate the pts point cloud by a specific angle.\n",
    "    \n",
    "    axis = 0 : rotate around x-axis\n",
    "    axis = 1 : rotate around y-axis\n",
    "    axis = 2 : rotate around z-axis\n",
    "    \n",
    "    angle_degree : how much degree you want to rotate the point cloud.\n",
    "    \n",
    "    Algorithm comes in: https://uk.mathworks.com/matlabcentral/answers/123763-how-to-rotate-entire-3d-data-with-x-y-z-values-along-a-particular-axis-say-x-axis\n",
    "    \"\"\"\n",
    "    if axis == 0: \n",
    "        return rotate_by_x(pts, angle_degree)\n",
    "    if axis == 1: \n",
    "        return rotate_by_y(pts, angle_degree)\n",
    "    if axis == 2: \n",
    "        return rotate_by_z(pts, angle_degree)\n",
    "    \n",
    "def rotate_by_x(pts, angle_degree=30):\n",
    "    import numpy as np\n",
    "    degree = np.deg2rad(angle_degree)\n",
    "    res = np.empty(pts.shape)\n",
    "    for index, point in enumerate(pts):\n",
    "        new_point = np.zeros(3)\n",
    "        new_point[0] = point[0] # x\n",
    "        new_point[1] = point[1]*np.cos(degree) - point[2]*np.sin(degree) # y\n",
    "        new_point[2] = point[1]*np.sin(degree) + point[2]*np.cos(degree) # z\n",
    "        res[index] = new_point\n",
    "        \n",
    "    return res\n",
    "\n",
    "    \n",
    "def rotate_by_y(pts, angle_degree=30):\n",
    "    import numpy as np\n",
    "    degree = np.deg2rad(angle_degree)\n",
    "    res = np.empty(pts.shape)\n",
    "    for index, point in enumerate(pts):\n",
    "        new_point = np.zeros(3)\n",
    "        new_point[0] = point[0]*np.cos(degree) + point[2]*np.sin(degree) # x\n",
    "        new_point[1] = point[1] # y\n",
    "        new_point[2] = point[2]*np.cos(degree) - point[0]*np.sin(degree) # z\n",
    "        res[index] = new_point\n",
    "        \n",
    "    return res\n",
    "\n",
    "    \n",
    "def rotate_by_z(pts, angle_degree=30):\n",
    "    import numpy as np\n",
    "    degree = np.deg2rad(angle_degree)\n",
    "    res = np.empty(pts.shape)\n",
    "    for index, point in enumerate(pts):\n",
    "        new_point = np.zeros(3)\n",
    "        new_point[0] = point[0]*np.cos(degree) - point[1]*np.sin(degree) # x\n",
    "        new_point[1] = point[0]*np.sin(degree) + point[1]*np.cos(degree) # y\n",
    "        new_point[2] = point[2] # z\n",
    "        res[index] = new_point\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(xyz, filename='xyz'):\n",
    "    import os\n",
    "    file = open(os.path.join(os.getcwd(), filename + \".pts\"), \"w\") \n",
    "    \n",
    "    for point in xyz:\n",
    "        st = \"\"\n",
    "        for item in point:\n",
    "            st += str(item) + \" \"\n",
    "        file.write(st.strip() + \"\\n\")\n",
    "\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rotated_point_cloud = rotate(my_point_cloud.xyz, angle_degree=180)\n",
    "write_to_file(rotated_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"pyntcloud_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11e217e10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_point_cloud_file = PyntCloud.from_file(os.path.join(os.getcwd(), \"xyz.pts\"), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "rotated_point_cloud_file.plot(point_size=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def voxelize3D(pts, dim=[1,1,1]):\n",
    "    \"\"\"\n",
    "    pts: receives .pts cloud point data. 2D array, arbitary sized X,Y,Z pairs. (We will only take x,y,z into account for now)\n",
    "    dim: dimensioin of output voxelized data\n",
    "    \n",
    "    This function will locate the grid cube and calculate the density of each cube.\n",
    "    The output will be normalized values.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1]>=3), \"pts file should contain at least x,y,z coordinate\"\n",
    "    assert(len(dim)==3), \"Please provide 3-d grid size like [32,32,32]\"\n",
    "    \n",
    "    # move all the axis to positive area.\n",
    "    minimum_val = [pts[0][0], pts[0][1], pts[0][2]]\n",
    "\n",
    "    # find the smallest \n",
    "    for pair in pts:\n",
    "        if pair[0] < minimum_val[0]:\n",
    "            minimum_val[0] = pair[0]\n",
    "        if pair[1] < minimum_val[1]:\n",
    "            minimum_val[1] = pair[1]\n",
    "        if pair[2] < minimum_val[2]:\n",
    "            minimum_val[2] = pair[2]\n",
    "            \n",
    "    # move it to first quadrant \n",
    "    rectified_pts = np.empty(pts.shape)\n",
    "    for index, pair in enumerate(pts):\n",
    "        point = np.zeros(3)\n",
    "        point[0] = pair[0] - minimum_val[0]\n",
    "        point[1] = pair[1] - minimum_val[1]\n",
    "        point[2] = pair[2] - minimum_val[2]\n",
    "        rectified_pts[index] = point\n",
    "    \n",
    "    # biggest value in each axis \n",
    "    maximum_val = pts[0][0]\n",
    "    \n",
    "    for pair in rectified_pts:\n",
    "        for val in pair:\n",
    "            if val > maximum_val:\n",
    "                maximum_val = val\n",
    "     \n",
    "    # normalize all the axises to (0,1)\n",
    "    normalized_pts = rectified_pts/maximum_val\n",
    "    \n",
    "    x_grid_length = 1/dim[0]\n",
    "    y_grid_length = 1/dim[1]\n",
    "    z_grid_length = 1/dim[2]\n",
    "    \n",
    "    output = np.zeros((dim[0],dim[1],dim[2]))\n",
    "    \n",
    "    epsilon = 0.000000000001 # we will have at least a 1.0 value which will exceed the index of grid\n",
    "    # we can use a relativly small value to escape that to fit our data\n",
    "    \n",
    "    max_volume_size = 0\n",
    "    \n",
    "    for pair in normalized_pts:\n",
    "        x_loc = int(pair[0]/(x_grid_length + epsilon))\n",
    "        y_loc = int(pair[1]/(y_grid_length + epsilon))\n",
    "        z_loc = int(pair[2]/(z_grid_length + epsilon))\n",
    "        if output[x_loc, y_loc, z_loc] is None:\n",
    "            output[x_loc, y_loc, z_loc] = 1\n",
    "        else:\n",
    "            output[x_loc, y_loc, z_loc] += 1\n",
    "        \n",
    "        if output[x_loc, y_loc, z_loc] > max_volume_size:\n",
    "            max_volume_size = output[x_loc, y_loc, z_loc]\n",
    "    \n",
    "    output = output/max_volume_size    \n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ./voxel_grid_plot.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_voxelgrid(voxelgrid,\n",
    "                   output_name=None,\n",
    "                   cmap=\"Oranges\",\n",
    "                   axis=True,\n",
    "                   width=800,\n",
    "                   height=600):\n",
    "\n",
    "    scaled_shape = voxelgrid.shape\n",
    "\n",
    "    vector = voxelgrid\n",
    "    points = np.argwhere(vector) * scaled_shape\n",
    "\n",
    "    s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    rgb = s_m.to_rgba(vector.reshape(-1)[vector.reshape(-1) > 0])\n",
    "\n",
    "    camera_position = points.max(0) + abs(points.max(0))\n",
    "\n",
    "    look = points.mean(0)\n",
    "\n",
    "    if axis:\n",
    "        axis_size = points.ptp() * 1.5\n",
    "    else:\n",
    "        axis_size = 0\n",
    "\n",
    "    placeholders = {}\n",
    "\n",
    "    placeholders[\"POINTS_X_PLACEHOLDER\"] = points[:, 0].tolist()\n",
    "    placeholders[\"POINTS_Y_PLACEHOLDER\"] = points[:, 1].tolist()\n",
    "    placeholders[\"POINTS_Z_PLACEHOLDER\"] = points[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"R_PLACEHOLDER\"] = rgb[:, 0].tolist()\n",
    "    placeholders[\"G_PLACEHOLDER\"] = rgb[:, 1].tolist()\n",
    "    placeholders[\"B_PLACEHOLDER\"] = rgb[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"S_x_PLACEHOLDER\"] = scaled_shape[0]\n",
    "    placeholders[\"S_y_PLACEHOLDER\"] = scaled_shape[1]\n",
    "    placeholders[\"S_z_PLACEHOLDER\"] = scaled_shape[2]\n",
    "\n",
    "    placeholders[\"CAMERA_X_PLACEHOLDER\"] = camera_position[0]\n",
    "    placeholders[\"CAMERA_Y_PLACEHOLDER\"] = camera_position[1]\n",
    "    placeholders[\"CAMERA_Z_PLACEHOLDER\"] = camera_position[2]\n",
    "\n",
    "    placeholders[\"LOOK_X_PLACEHOLDER\"] = look[0]\n",
    "    placeholders[\"LOOK_Y_PLACEHOLDER\"] = look[1]\n",
    "    placeholders[\"LOOK_Z_PLACEHOLDER\"] = look[2]\n",
    "\n",
    "    placeholders[\"AXIS_SIZE_PLACEHOLDER\"] = axis_size\n",
    "\n",
    "    placeholders[\"N_VOXELS_PLACEHOLDER\"] = sum(vector.reshape(-1) > 0)\n",
    "\n",
    "    if output_name is None:\n",
    "        output_name = \"plotVG.html\"\n",
    "\n",
    "    BASE_PATH = os.getcwd()\n",
    "    src = \"{}/{}\".format(BASE_PATH, \"voxelgrid.html\")\n",
    "    dst = \"{}/{}\".format(os.getcwd(), output_name)\n",
    "\n",
    "    with open(src, \"r\") as inp, open(dst, \"w\") as out:\n",
    "        for line in inp:\n",
    "            for key, val in placeholders.items():\n",
    "                if key in line:\n",
    "                    line = line.replace(key, str(val))\n",
    "            out.write(line)\n",
    "\n",
    "    return IFrame(output_name, width=width, height=height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoom in a little bit, seems like some visual bugs inside the code if you have a large set of points\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11e1cea90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Zoom in a little bit, seems like some visual bugs inside the code if you have a large set of points\")\n",
    "vox = voxelize3D(my_point_cloud.xyz, dim=[48,48,48])\n",
    "plot_voxelgrid(vox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squeeze_pts(pts, axis=0, percentage=10):\n",
    "    if axis==0:\n",
    "        return squeeze_by_x(pts, percentage)\n",
    "    if axis==1:\n",
    "        return squeeze_by_y(pts, percentage)\n",
    "    if axis==2:\n",
    "        return squeeze_by_z(pts, percentage)\n",
    "    \n",
    "def squeeze_by_x(pts, percentage):\n",
    "    \n",
    "    v = (100-percentage)/100\n",
    "    output = np.empty(pts.shape)\n",
    "    \n",
    "    assert(v>=0 and v<=1), \"Percentage should between 0 and 100\"\n",
    "    \n",
    "    for index, point in enumerate(pts):\n",
    "        output[index] = point\n",
    "        output[index][0] = output[index][0]*v\n",
    "        \n",
    "    return output\n",
    "    \n",
    "def squeeze_by_y(pts, percentage):\n",
    "    \n",
    "    v = (100-percentage)/100\n",
    "    output = np.empty(pts.shape)\n",
    "    \n",
    "    assert(v>=0 and v<=1), \"Percentage should between 0 and 100\"\n",
    "    \n",
    "    for index, point in enumerate(pts):\n",
    "        output[index] = point\n",
    "        output[index][1] = output[index][1]*v\n",
    "        \n",
    "    return output\n",
    "\n",
    "    \n",
    "def squeeze_by_z(pts, percentage):\n",
    "    \n",
    "    v = (100-percentage)/100\n",
    "    output = np.empty(pts.shape)\n",
    "    \n",
    "    assert(v>=0 and v<=1), \"Percentage should between 0 and 100\"\n",
    "    \n",
    "    for index, point in enumerate(pts):\n",
    "        output[index] = point\n",
    "        output[index][2] = output[index][2]*v\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squeezed_pts = squeeze_pts(my_point_cloud.xyz, axis=1, percentage=30)\n",
    "write_to_file(squeezed_pts, filename=\"sq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"pyntcloud_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11e1e09e8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_point_cloud_file = PyntCloud.from_file(os.path.join(os.getcwd(), \"sq.pts\"), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "rotated_point_cloud_file.plot(point_size=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(pts, noise_percentage=0.05):\n",
    "    \n",
    "    noise_size = int(noise_percentage*len(pts))\n",
    "    \n",
    "    max_x = min_x = pts[0][0]\n",
    "    max_y = min_y = pts[0][1]\n",
    "    max_z = min_z = pts[0][2]\n",
    "    \n",
    "    for point in pts:\n",
    "        if max_x < point[0]:\n",
    "            max_x = point[0]\n",
    "        if max_y < point[1]:\n",
    "            max_y = point[1]\n",
    "        if max_z < point[2]:\n",
    "            max_z = point[2]\n",
    "            \n",
    "        if min_x < point[0]:\n",
    "            min_x = point[0]\n",
    "        if min_y < point[1]:\n",
    "            min_y = point[1]\n",
    "        if min_y < point[2]:\n",
    "            min_y = point[2]\n",
    "            \n",
    "    import numpy as np\n",
    "    \n",
    "    noise_pts = np.empty((noise_size, 3))\n",
    "    noise_x = np.random.randn(noise_size)*(max_x - min_x)/2\n",
    "    noise_y = np.random.randn(noise_size)*(max_y - min_y)/2\n",
    "    noise_z = np.random.randn(noise_size)*(max_z - min_z)/2\n",
    "    \n",
    "    for i, _ in enumerate(range(noise_size)):\n",
    "        pos = np.random.randint(0,len(pts)-1)\n",
    "        noise_pts[i][0] = pts[pos][0] + noise_x[i]\n",
    "        noise_pts[i][1] = pts[pos][1] + noise_y[i]\n",
    "        noise_pts[i][2] = pts[pos][2] + noise_z[i]\n",
    "        \n",
    "    return np.concatenate((pts, noise_pts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noisy_pts = add_noise(my_point_cloud.xyz)\n",
    "write_to_file(noisy_pts, filename=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"pyntcloud_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11e703518>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_pts_file = PyntCloud.from_file(os.path.join(os.getcwd(), \"no.pts\"), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "noisy_pts_file.plot(point_size=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy voxelized point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x111c98f28>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vox = voxelize3D(noisy_pts, dim=[48,48,48])\n",
    "plot_voxelgrid(vox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PyntCloud.get_filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_cates(labels):\n",
    "    cates = []\n",
    "    for l in labels:\n",
    "        if l not in cates:\n",
    "            cates.append(l)\n",
    "    return cates\n",
    "\n",
    "def data_onehot_encode(labels):\n",
    "    \"\"\"\n",
    "    Recieves an array of labels.\n",
    "    \"\"\"\n",
    "    cates = label_cates(labels)\n",
    "    # one-hot\n",
    "    onehot = []\n",
    "    for l in labels:\n",
    "        x = np.zeros(len(cates))\n",
    "        x[cates.index(l)] = 1.0\n",
    "        onehot.append(x)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_reshape(data):\n",
    "    \"\"\"\n",
    "    Will read and voxelize the data\n",
    "    \"\"\"\n",
    "    x_reshaped = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if i % 20 == 0:\n",
    "            print(\"Process: \", str('{0:.2f}'.format(i/len(data))) + \"%\", end='\\r')\n",
    "        my_point_cloud = PyntCloud.from_file(data[i], sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "        vox = voxelize3D(my_point_cloud.xyz, [32,32,32])\n",
    "        vox_chan = np.array(vox).reshape(vox.shape + (1,))\n",
    "        x_reshaped.append(vox_chan)\n",
    "        \n",
    "    print(\"Process: \", \" 100% \")        \n",
    "    return x_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_swap(data, index_a, index_b):\n",
    "    temp = data[index_a]\n",
    "    data[index_a] = data[index_b]\n",
    "    data[index_b] = temp\n",
    "    return data\n",
    "\n",
    "def data_random_position(data):\n",
    "    import random\n",
    "    return random.randint(0,len(data)-1)\n",
    "\n",
    "def data_shuffling(data, label):\n",
    "    for i in range(len(data)):\n",
    "        target = data_random_position(data)\n",
    "        data = data_swap(data, i, target)\n",
    "        label = data_swap(label, i, target)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled_data_raw, shuffled_label_raw = data_shuffling(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process:   100% \n"
     ]
    }
   ],
   "source": [
    "shuffled_data = data_reshape(shuffled_data_raw)\n",
    "shuffled_label = data_onehot_encode(shuffled_label_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_classes = label_cates(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 1)\n",
      "[ 0.  0.  1.  0.]\n",
      "4\n",
      "['rocket', 'earphone', 'cap', 'bag']\n"
     ]
    }
   ],
   "source": [
    "print(shuffled_data[100].shape)\n",
    "print(shuffled_label[100])\n",
    "print(len(shuffled_label[100]))\n",
    "print(label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def cnn_model(x_train_data, label_size, keep_rate=0.7, seed=None):\n",
    "    \n",
    "    with tf.name_scope(\"layer_a\"):\n",
    "        # conv => 32*32*32\n",
    "        conv1 = tf.layers.conv3d(inputs=x_train_data, filters=16, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu, name=\"conv1\")\n",
    "        # conv => 32*32*32\n",
    "        conv2 = tf.layers.conv3d(inputs=conv1, filters=32, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu, name=\"conv2\")\n",
    "        # pool => 16*16*16\n",
    "        pool3 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[2, 2, 2], strides=2, name=\"pool3\")\n",
    "        \n",
    "    with tf.name_scope(\"layer_b\"):\n",
    "        # conv => 16*16*16\n",
    "        conv4 = tf.layers.conv3d(inputs=pool3, filters=64, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu, name=\"conv4\")\n",
    "        # conv => 16*16*16\n",
    "        conv5 = tf.layers.conv3d(inputs=conv4, filters=128, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu, name=\"conv5\")\n",
    "        # pool => 8*8*8\n",
    "        pool6 = tf.layers.max_pooling3d(inputs=conv5, pool_size=[2, 2, 2], strides=2, name=\"pool6\")\n",
    "        \n",
    "    with tf.name_scope(\"layer_c\"):\n",
    "        # conv => 8*8*8\n",
    "        conv7 = tf.layers.conv3d(inputs=pool6, filters=256, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu, name=\"conv7\")\n",
    "        # conv => 8*8*8\n",
    "        conv8 = tf.layers.conv3d(inputs=conv7, filters=512, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu, name=\"conv8\")\n",
    "        # pool => 4*4*4\n",
    "        pool9 = tf.layers.max_pooling3d(inputs=conv8, pool_size=[2, 2, 2], strides=2, name=\"pool9\")\n",
    "        \n",
    "    with tf.name_scope(\"batch_norm\"):\n",
    "        cnn3d_bn = tf.layers.batch_normalization(inputs=pool9, training=True, name=\"bn\")\n",
    "        \n",
    "    with tf.name_scope(\"fully_con\"):\n",
    "        flattening = tf.reshape(cnn3d_bn, [-1, 4*4*4*512])\n",
    "        dense = tf.layers.dense(inputs=flattening, units=1024, activation=tf.nn.relu, name=\"full_con\")\n",
    "        # (1-keep_rate) is the probability that the node will be kept\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=keep_rate, training=True, name=\"dropout\")\n",
    "        \n",
    "    with tf.name_scope(\"y_conv\"):\n",
    "        y_conv = tf.layers.dense(inputs=dropout, units=label_size, name=\"y_pred\")\n",
    "    \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(x_train_data, y_train_data, x_test_data, y_test_data, learning_rate=0.1, keep_rate=0.7, epochs=10, batch_size=128, using_gpu=False):\n",
    "\n",
    "    if using_gpu:\n",
    "        device_name = '/gpu:1' \n",
    "    else:\n",
    "        device_name = '/cpu:0'\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    with tf.device(device_name):\n",
    "        with tf.name_scope('inputs'):\n",
    "            x_input = tf.placeholder(tf.float32, shape=[None, 32, 32, 32, 1], name=\"x_input\")\n",
    "            y_input = tf.placeholder(tf.float32, shape=[None, len(shuffled_label[0])], name=\"y_input\") \n",
    "\n",
    "        prediction = cnn_model(x_input, len(y_train_data[0]), keep_rate, seed=1)\n",
    "        tf.add_to_collection(\"logits\", prediction)\n",
    "        \n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_input), name=\"cross_entropy\")\n",
    "                              \n",
    "        with tf.name_scope(\"training\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "            tf.add_to_collection(\"optimizer\", optimizer)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'), name=\"acc\")\n",
    "        \n",
    "   \n",
    "    if using_gpu:\n",
    "        # GPU using BFC\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allocator_type = 'BFC'\n",
    "        sess =  tf.Session(config= config)\n",
    "            \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    import datetime\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    iterations_train = int(len(x_train_data)/batch_size) + 1\n",
    "    iterations_test = int(len(x_test_data)/batch_size) + 1\n",
    "    # run epochs\n",
    "    for epoch in range(epochs):\n",
    "        start_time_epoch = datetime.datetime.now()\n",
    "        print('Epoch', epoch, 'started')\n",
    "        \n",
    "        # mini batch\n",
    "        for itr in range(iterations_train):\n",
    "            mini_batch_x = x_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "            mini_batch_y = y_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "            _optimizer, _cost = sess.run([optimizer, cost], feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "            print('\\tLost for', itr, \"/\", iterations_train, _cost, end='\\r')\n",
    "\n",
    "        import os\n",
    "        model_path = os.path.join(os.getcwd(), 'model', 'model')\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, model_path, global_step=epoch)\n",
    "            \n",
    "        acc = 0\n",
    "        # mini batch\n",
    "        for itr in range(iterations_test):\n",
    "            mini_batch_x = x_test_data[itr*batch_size: (itr+1)*batch_size]\n",
    "            mini_batch_y = y_test_data[itr*batch_size: (itr+1)*batch_size]\n",
    "            _acc = sess.run(accuracy, feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "            acc += _acc\n",
    "        \n",
    "        end_time_epoch = datetime.datetime.now()\n",
    "        print('Testing Set Accuracy:',acc/iterations_test, ' Time elapse: ', str(end_time_epoch - start_time_epoch)) \n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    print('Time elapse: ', str(end_time - start_time))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cross_validation:\n",
    "    \n",
    "    def __init__(self, data, label, fold=5):\n",
    "        self.data = np.asarray(data)\n",
    "        self.label = np.asarray(label)\n",
    "        assert(len(self.data) == len(self.label))\n",
    "        self.fold = fold\n",
    "        self.current_itr = 0;\n",
    "        self.sample_in_fold = int(len(data)/fold)\n",
    "\n",
    "    def next_bunch(self):\n",
    "        \"\"\"\n",
    "        Return x_train, y_train, x_test, y_test\n",
    "        \"\"\"\n",
    "        assert(self.current_itr < self.fold)\n",
    "        start = self.current_itr * self.sample_in_fold\n",
    "        end = (self.current_itr + 1) * self.sample_in_fold\n",
    "        self.current_itr += 1\n",
    "        if start == 0:\n",
    "            return self.data[end:], self.label[end:], self.data[:end], self.label[:end]\n",
    "        if end >= len(self.data):\n",
    "            return self.data[:start], self.label[:start], self.data[start:], self.label[start:]\n",
    "\n",
    "        return np.concatenate((self.data[:start],self.data[end:])), np.concatenate((self.label[:start],self.label[end:])), self.data[start : end], self.label[start : end]\n",
    "\n",
    "    def have_next(self):\n",
    "        if self.current_itr != self.fold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  1.]\n",
      "bag\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x12f49fb00>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = cross_validation(shuffled_data, shuffled_label, fold=5)\n",
    "\n",
    "_x_train, _y_train, _x_test, _y_test = cv.next_bunch()\n",
    "\n",
    "print(_y_train[15])\n",
    "print(label[15])\n",
    "plot_voxelgrid(_x_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n",
      "\tLost for 14 / 15 59.7611\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'iterations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-235bfb398c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musing_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-ba5dc7af2bad>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(x_train_data, y_train_data, x_test_data, y_test_data, learning_rate, keep_rate, epochs, batch_size, using_gpu)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mend_time_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing Set Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' Time elapse: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time_epoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iterations' is not defined"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(shuffled_data, shuffled_label, fold=5)\n",
    "_x_train, _y_train, _x_test, _y_test = cv.next_bunch()\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_neural_network(_x_train[:], _y_train[:], _x_test[:], _y_test[:], learning_rate=0.005, batch_size=16, epochs=1, using_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 137.9825592   -29.36825562  -10.86287117  -42.96081543]\n",
      " [-192.73469543  -11.08665466 -293.6958313   162.68659973]\n",
      " [  31.17004776 -289.51126099  327.64489746  -92.02298737]\n",
      " [-460.47531128   48.48471069 -389.76831055  357.96966553]\n",
      " [-314.02828979  409.01141357 -487.20553589  109.7181015 ]\n",
      " [  86.88998413   -9.37506104 -207.32275391   -9.14642811]\n",
      " [-153.13311768  204.51615906 -207.69502258  272.69924927]\n",
      " [ 115.52864075 -154.88525391  -64.5151062   -90.84585571]\n",
      " [-121.49363708 -587.25689697  310.98883057 -264.64978027]\n",
      " [-239.32524109  125.37471008 -274.02114868   72.90899658]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "new_graph = tf.Graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True) # allow passing gpu-trained model to a cpu machine\n",
    "with tf.Session(graph=new_graph, config=config) as sess:\n",
    "\n",
    "    import os\n",
    "    model_fqn = os.path.join(os.getcwd(), 'model', \"model-0\")\n",
    "    saver = tf.train.import_meta_graph(model_fqn + \".meta\")\n",
    "    saver.restore(sess, model_fqn)\n",
    "\n",
    "    model_graph = tf.get_default_graph()\n",
    "    \n",
    "    cv = cross_validation(shuffled_data, shuffled_label, fold=5)\n",
    "    _x_train, _y_train, _x_test, _y_test = cv.next_bunch()\n",
    "\n",
    "    _x_input = model_graph.get_tensor_by_name('inputs/x_input:0')\n",
    "    _y_input = model_graph.get_tensor_by_name('inputs/y_input:0')\n",
    "\n",
    "    acc = model_graph.get_tensor_by_name('acc:0')\n",
    "    logits = tf.get_collection(\"logits\")[0]\n",
    "    \n",
    "    oprimizer = model_graph.get_collection('optimizer')\n",
    "\n",
    "    _ = sess.run(logits, feed_dict={_x_input: _x_test[:10], _y_input: _y_test[:300]})\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
