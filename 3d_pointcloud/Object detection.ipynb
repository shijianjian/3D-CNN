{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# piont cloud separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pyntcloud import PyntCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ranges(point_cloud):\n",
    "\n",
    "    x_max = x_min = point_cloud[0][0]\n",
    "    y_max = y_min = point_cloud[0][1]\n",
    "    z_max = z_min = point_cloud[0][2]\n",
    "    \n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "        if coor[0] > x_max:\n",
    "            x_max = coor[0]\n",
    "        if coor[0] < x_min:\n",
    "            x_min = coor[0]\n",
    "        if coor[1] > y_max:\n",
    "            y_max = coor[1]\n",
    "        if coor[1] < y_min:\n",
    "            y_min = coor[1]\n",
    "        if coor[2] > z_max:\n",
    "            z_max = coor[2]\n",
    "        if coor[2] < z_min:\n",
    "            z_min = coor[2]\n",
    "            \n",
    "    return ((x_min, x_max), (y_min, y_max), (z_min, z_max))\n",
    "\n",
    "\n",
    "def segment_points(point_cloud, target_ranges):\n",
    "    \"\"\"\n",
    "    Recives orig point cloud data and a tuple of boundary tuples like:\n",
    "        ((x_min, x_max), (y_min, y_max), (z_min, z_max))\n",
    "    \"\"\"\n",
    "    \n",
    "    target_x_range, target_y_range, target_z_range = target_ranges[0], target_ranges[1], target_ranges[2]\n",
    "    \n",
    "    points = []\n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "\n",
    "        if coor[0] >= target_x_range[0] and coor[0] <= target_x_range[1]:\n",
    "            if coor[1] >= target_y_range[0] and coor[1] <= target_y_range[1]:\n",
    "                if coor[2] >= target_z_range[0] and coor[2] <= target_z_range[1]:\n",
    "                    points.append(coor)\n",
    "                    \n",
    "    return np.asarray(points)\n",
    "\n",
    "\n",
    "def norm_point(point_cloud):\n",
    "    \n",
    "    ((x_min, x_max), (y_min, y_max), (z_min, z_max)) = find_ranges(point_cloud)\n",
    "    \n",
    "    biggest_value = np.max(np.asarray([x_max - x_min, y_max - y_min, z_max - z_min])) + 0.000000001\n",
    "    \n",
    "    normalized_points = []\n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "        row = np.empty(len(coor))\n",
    "        row[0] = (coor[0] - x_min)/biggest_value\n",
    "        row[1] = (coor[1] - y_min)/biggest_value\n",
    "        row[2] = (coor[2] - z_min)/biggest_value\n",
    "        normalized_points.append(row)\n",
    "        \n",
    "    return np.asarray(normalized_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_point(point_cloud, cube_size=(3,3,3), overall_size=[32,32,32], stride=(1,1,1), filter_points_num=None):\n",
    "    \"\"\"\n",
    "    It will return the target segment of point cloud.\n",
    "    \"\"\"\n",
    "    threshold = 0\n",
    "    \n",
    "    if filter_points_num is not None:\n",
    "        threshold = filter_points_num\n",
    "    \n",
    "    norm_point_cloud = norm_point(point_cloud)\n",
    "    xyz_length = 1/np.asarray(overall_size)\n",
    "\n",
    "    segmentations = []\n",
    "    areas = []\n",
    "    \n",
    "    x_num = int((overall_size[0] - cube_size[0]) / stride[0] + 1)\n",
    "    y_num = int((overall_size[1] - cube_size[1]) / stride[1] + 1)\n",
    "    z_num = int((overall_size[2] - cube_size[2]) / stride[2] + 1)\n",
    "    \n",
    "    covers_all = (x_num == (overall_size[0] - cube_size[0]) / stride[0] + 1) and (y_num == (overall_size[1] - cube_size[1]) / stride[1] + 1) and (z_num == (overall_size[2] - cube_size[2]) / stride[2] + 1)\n",
    "\n",
    "    for _x in range(x_num):\n",
    "        for _y in range(y_num):\n",
    "            for _z in range(z_num):\n",
    "                target_area = ((_x*stride[0], _x*stride[0] + cube_size[0]), (_y*stride[1], _y*stride[1] + cube_size[1]), (_z*stride[2], _z*stride[2] + cube_size[2]))\n",
    "    \n",
    "                target_x_range = (target_area[0][0]*xyz_length[0], target_area[0][1]*xyz_length[0])\n",
    "                target_y_range = (target_area[1][0]*xyz_length[1], target_area[1][1]*xyz_length[1])\n",
    "                target_z_range = (target_area[2][0]*xyz_length[2], target_area[2][1]*xyz_length[2])\n",
    "                \n",
    "                segmented_point_cloud = segment_points(norm_point_cloud, (target_x_range, target_y_range, target_z_range))\n",
    "\n",
    "                if len(segmented_point_cloud) > threshold:\n",
    "                    \n",
    "                    segmentations.append(segmented_point_cloud)\n",
    "                    \n",
    "                    areas.append(target_area)\n",
    "                    \n",
    "    # Add in the last piece of cube ()               \n",
    "    if not covers_all: \n",
    "        target_area = ((x_num*stride[0], x_num*stride[0] + cube_size[0]), (y_num*stride[1], y_num*stride[1] + cube_size[1]), (z_num*stride[2], z_num*stride[2] + cube_size[2]))\n",
    "    \n",
    "        target_x_range = (target_area[0][0]*xyz_length[0], target_area[0][1]*xyz_length[0])\n",
    "        target_y_range = (target_area[1][0]*xyz_length[1], target_area[1][1]*xyz_length[1])\n",
    "        target_z_range = (target_area[2][0]*xyz_length[2], target_area[2][1]*xyz_length[2])\n",
    "\n",
    "        segmented_point_cloud = segment_points(norm_point_cloud, (target_x_range, target_y_range, target_z_range))\n",
    "\n",
    "        if len(segmented_point_cloud) > threshold:\n",
    "            \n",
    "            segmentations.append(segmented_point_cloud)   \n",
    "            \n",
    "            areas.append(target_area)\n",
    "    \n",
    "    return {'data': segmentations, 'area': areas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "my_point_cloud = PyntCloud.from_file(os.path.join(os.getcwd(), 'ttt2.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = traverse_point(my_point_cloud.xyz, cube_size=(5,5,5), overall_size=[32,32,32], stride=(1,1,1), filter_points_num=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelize3D(pts, dim=[1,1,1]):\n",
    "    \"\"\"\n",
    "    pts: receives .pts cloud point data. 2D array, arbitary sized X,Y,Z pairs. (We will only take x,y,z into account for now)\n",
    "    dim: dimensioin of output voxelized data\n",
    "    \n",
    "    This function will locate the grid cube and calculate the density of each cube.\n",
    "    The output will be normalized values.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1]>=3), \"pts file should contain at least x,y,z coordinate\"\n",
    "    assert(len(dim)==3), \"Please provide 3-d grid size like [32,32,32]\"\n",
    "    \n",
    "    if len(pts) > 1:\n",
    "        # move all the axis to positive area.\n",
    "        minimum_val = [pts[0][0], pts[0][1], pts[0][2]]\n",
    "\n",
    "        # find the smallest \n",
    "        for pair in pts:\n",
    "            if pair[0] < minimum_val[0]:\n",
    "                minimum_val[0] = pair[0]\n",
    "            if pair[1] < minimum_val[1]:\n",
    "                minimum_val[1] = pair[1]\n",
    "            if pair[2] < minimum_val[2]:\n",
    "                minimum_val[2] = pair[2]\n",
    "\n",
    "        # move it to first quadrant \n",
    "        rectified_pts = np.empty(pts.shape)\n",
    "        for index, pair in enumerate(pts):\n",
    "            point = np.zeros(3)\n",
    "            point[0] = pair[0] - minimum_val[0]\n",
    "            point[1] = pair[1] - minimum_val[1]\n",
    "            point[2] = pair[2] - minimum_val[2]\n",
    "            rectified_pts[index] = point\n",
    "\n",
    "        # biggest value in each axis \n",
    "        maximum_val = pts[0][0]\n",
    "\n",
    "        for pair in rectified_pts:\n",
    "            for val in pair:\n",
    "                if val > maximum_val:\n",
    "                    maximum_val = val\n",
    "\n",
    "        # normalize all the axises to (0,1)\n",
    "        normalized_pts = rectified_pts/maximum_val\n",
    "    \n",
    "    else:\n",
    "        # in case there is just one point\n",
    "        normalized_pts = pts\n",
    "    \n",
    "    x_grid_length = 1/dim[0]\n",
    "    y_grid_length = 1/dim[1]\n",
    "    z_grid_length = 1/dim[2]\n",
    "    \n",
    "    output = np.zeros((dim[0],dim[1],dim[2]))\n",
    "    \n",
    "    epsilon = 0.000000000001 # we will have at least a 1.0 value which will exceed the index of grid\n",
    "    # we can use a relativly small value to escape that to fit our data\n",
    "    \n",
    "    max_volume_size = 0\n",
    "    \n",
    "    for pair in normalized_pts:\n",
    "        x_loc = int(pair[0]/(x_grid_length + epsilon))\n",
    "        y_loc = int(pair[1]/(y_grid_length + epsilon))\n",
    "        z_loc = int(pair[2]/(z_grid_length + epsilon))\n",
    "        if output[x_loc, y_loc, z_loc] is None:\n",
    "            output[x_loc, y_loc, z_loc] = 1\n",
    "        else:\n",
    "            output[x_loc, y_loc, z_loc] += 1\n",
    "        \n",
    "        if output[x_loc, y_loc, z_loc] > max_volume_size:\n",
    "            max_volume_size = output[x_loc, y_loc, z_loc]\n",
    "    \n",
    "    output = output/max_volume_size    \n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13367\n",
      "1331\n",
      "1827\n"
     ]
    }
   ],
   "source": [
    "print(len(my_point_cloud.xyz))\n",
    "print(len(segs['data'][0]))\n",
    "print(len(segs['area']))\n",
    "vox_segs = []\n",
    "whole_vox = voxelize3D(my_point_cloud.xyz, dim=[32,32,32])\n",
    "whole_vox = whole_vox.reshape(whole_vox.shape + (1,))\n",
    "\n",
    "for idx, value in enumerate(segs['data']):\n",
    "    vox = voxelize3D(value, dim=[32,32,32])\n",
    "    vox_chan = np.array(vox).reshape(vox.shape + (1,))\n",
    "    vox_segs.append(vox_chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create hdf5\n",
    "hdf5_path = os.path.join(os.getcwd(), 'h5dataset', 'big_shuffled_sets.h5')\n",
    "hdf5_file = h5py.File(hdf5_path, mode='r')\n",
    "\n",
    "b_data = hdf5_file.get('voxels')\n",
    "b_labels = hdf5_file.get('labels')\n",
    "b_label_ref = hdf5_file.get('label_ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one hot indexes\n",
    "\n",
    "def predict(voxels, output_format='weights', device_name='cpu:0'):\n",
    "    \"\"\"\n",
    "    Output_format can be 'weights' or 'probs'.\n",
    "    \n",
    "    'weights' will output the weight straight away which can be treated as confident.\n",
    "    \n",
    "    'probs' will output the corresponding probabilities for each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    model_path = os.path.join(os.getcwd(), 'trained_model', 'model-2')\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "        with tf.device(device_name):\n",
    "\n",
    "            saver = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "            saver.restore(sess, model_path)\n",
    "\n",
    "            graph = tf.get_default_graph()\n",
    "            x_input = graph.get_tensor_by_name('inputs/x_input:0')\n",
    "            y_input = graph.get_tensor_by_name('inputs/y_input:0')\n",
    "            pred = graph.get_collection('logits')\n",
    "            accuracy = graph.get_tensor_by_name('acc:0')\n",
    "            \n",
    "            res = []\n",
    "            \n",
    "            if output_format is 'weights':\n",
    "                for _, val in enumerate(voxels):\n",
    "                    y = sess.run(pred, feed_dict={x_input: [val]})\n",
    "                    res.append(np.asarray(y[0]))\n",
    "                return res\n",
    "            \n",
    "            elif output_format is 'probs':\n",
    "                for _, val in enumerate(voxels):\n",
    "                    y = sess.run(pred, feed_dict={x_input: [val]})\n",
    "                    res.append(tf.nn.softmax(np.asarray(y[0])).eval())\n",
    "                return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(vox_segs, output_format='weights', device_name='cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_prob(probs, cube_labels, threshold=0.7):\n",
    "    li = dict()\n",
    "    cubes = dict()\n",
    "    for _, val in enumerate(probs):\n",
    "        _max = np.argmax(val)\n",
    "        if val[0][_max] > threshold:\n",
    "            if _max not in li:\n",
    "                li.update({_max:1})\n",
    "                m_li = list()\n",
    "                m_li.append(cube_labels[_])\n",
    "                cubes.update({_max: m_li})\n",
    "            else:\n",
    "                li.update({_max: li[_max]+1})\n",
    "                m_li = cubes[_max]\n",
    "                m_li.append(cube_labels[_])\n",
    "                cubes.update({_max: m_li})\n",
    "    return li, cubes\n",
    "\n",
    "def count_confidence(confidence, cube_labels, threshold=3):\n",
    "    li = dict()\n",
    "    cubes = dict()\n",
    "    for _, val in enumerate(confidence):\n",
    "        _max = np.argmax(val)\n",
    "        if val[0][_max] > threshold:\n",
    "            if _max not in li:\n",
    "                li.update({_max:1})\n",
    "                m_li = list()\n",
    "                m_li.append(cube_labels[_])\n",
    "                cubes.update({_max: m_li})\n",
    "            else:\n",
    "                li.update({_max: li[_max]+1})\n",
    "                m_li = cubes[_max]\n",
    "                m_li.append(cube_labels[_])\n",
    "                cubes.update({_max: m_li})\n",
    "    return li, cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_density(occ_list, upper=[32,32,32]):\n",
    "    res = np.zeros((upper[0], upper[1], upper[2]))/10\n",
    "    for occ_row in occ_list:\n",
    "        for _x in range(occ_row[0][1] - occ_row[0][0]):\n",
    "            for _y in range(occ_row[1][1] - occ_row[1][0]):\n",
    "                for _z in range(occ_row[2][1] - occ_row[2][0]):\n",
    "                    res[occ_row[0][0]+_x][occ_row[1][0]+_y][occ_row[2][0]+_z] = 1\n",
    "    return res\n",
    "\n",
    "def to_union(occ_list):\n",
    "    res = [[occ_list[0][0][0], occ_list[0][0][1]], [occ_list[0][1][0], occ_list[0][1][1]], [occ_list[0][2][0], occ_list[0][2][1]]]\n",
    "    for occ_row in occ_list:\n",
    "        if res[0][0] > occ_row[0][0]:\n",
    "            res[0][0] = occ_row[0][0]\n",
    "        if res[0][1] < occ_row[0][1]:\n",
    "            res[0][1] = occ_row[0][1]\n",
    "            \n",
    "        if res[1][0] > occ_row[1][0]:\n",
    "            res[1][0] = occ_row[1][0]\n",
    "        if res[1][1] < occ_row[1][1]:\n",
    "            res[1][1] = occ_row[1][1]\n",
    "            \n",
    "        if res[2][0] > occ_row[2][0]:\n",
    "            res[2][0] = occ_row[2][0]\n",
    "        if res[2][1] < occ_row[2][1]:\n",
    "            res[2][1] = occ_row[2][1]\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 60, 2: 91, 4: 133}\n",
      "[[1, 13], [4, 15], [6, 17]]\n",
      "[b'car' b'lamp' b'table' b'airplane' b'chair']\n"
     ]
    }
   ],
   "source": [
    "argmax, occ = count_confidence(confidence, segs['area'], threshold=4)\n",
    "print(argmax)\n",
    "print(to_union(occ[4]))\n",
    "print(b_label_ref.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1423\n"
     ]
    }
   ],
   "source": [
    "target = occ[1]\n",
    "for idx, v in enumerate(segs['area']):\n",
    "    if v in target:\n",
    "        print(idx, len(segs['data'][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(os.path.join(os.getcwd(),\"occ5out32.temp\"), \"w\") \n",
    "# file.write(str(segs))\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(xyz, filename='xyz'):\n",
    "    import os\n",
    "    file = open(os.path.join(os.getcwd(), filename + \".pts\"), \"w\") \n",
    "    \n",
    "    for point in xyz:\n",
    "        st = \"\"\n",
    "        for item in point:\n",
    "            st += str(item) + \" \"\n",
    "        file.write(st.strip() + \"\\n\")\n",
    "\n",
    "    file.close() \n",
    "    \n",
    "# write_to_file(segmented_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./voxel_grid_plot.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from matplotlib import pyplot as plt\n",
    "from pyntcloud import PyntCloud\n",
    "\n",
    "def plot_voxel_points(voxelgrid,\n",
    "                   point_cloud,\n",
    "                   output_name=None,\n",
    "                   cmap=\"Oranges\",\n",
    "                   axis=True,\n",
    "                   width=800,\n",
    "                   height=600):\n",
    "\n",
    "    # For Voxel Grid\n",
    "    scaled_shape = voxelgrid.shape\n",
    "\n",
    "    vector = voxelgrid\n",
    "    points = np.argwhere(vector)\n",
    "\n",
    "    s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    rgb = s_m.to_rgba(vector.reshape(-1)[vector.reshape(-1) > 0])\n",
    "\n",
    "    camera_position = points.max(0) + abs(points.max(0))\n",
    "\n",
    "    look = points.mean(0)\n",
    "\n",
    "    if axis:\n",
    "        axis_size = points.ptp() * 1.5\n",
    "    else:\n",
    "        axis_size = 0\n",
    "    \n",
    "    # For point cloud\n",
    "    filename = 'pyntcloud_plot'\n",
    "    \n",
    "    path = os.path.join(os.getcwd(), filename + \".pts\")\n",
    "    # Fit point cloud into Voxel Grid\n",
    "    (x_min, x_max), (y_min, y_max), (z_min, z_max) = find_ranges(point_cloud)\n",
    "    pts_camera_position = (point_cloud.max(0) + abs(point_cloud.max(0))).tolist()\n",
    "    new_point_cloud = np.empty(point_cloud.shape)\n",
    "    for idx, row in enumerate(point_cloud):\n",
    "        _r = np.empty(row.shape)\n",
    "        _r[0] = (row[0] - x_min)/(x_max - x_min)*scaled_shape[0]\n",
    "        _r[1] = (row[1] - y_min)/(y_max - y_min)*scaled_shape[1]\n",
    "        _r[2] = (row[2] - z_min)/(z_max - z_min)*scaled_shape[2]\n",
    "        new_point_cloud[idx] = _r\n",
    "    point_cloud = new_point_cloud\n",
    "    \n",
    "    # Orange by default\n",
    "    colors = np.repeat([[255, 125, 0]], point_cloud.shape[0], axis=0)\n",
    "    colors = colors.astype(np.uint8)\n",
    "    points_df = pd.DataFrame(point_cloud, columns=[\"x\", \"y\", \"z\"])\n",
    "    for n, i in enumerate([\"red\", \"green\", \"blue\"]):\n",
    "        points_df[i] = colors[:, n]\n",
    "    cloud = PyntCloud(points_df)\n",
    "    \n",
    "    ply_gen = cloud.to_file(\"{}.ply\".format(filename), also_save=[\"mesh\"])\n",
    "    \n",
    "#     look_at = cloud.xyz.mean(0).tolist()\n",
    "    \n",
    "\n",
    "    placeholders = {}\n",
    "\n",
    "    placeholders[\"POINTS_X_PLACEHOLDER\"] = points[:, 0].tolist()\n",
    "    placeholders[\"POINTS_Y_PLACEHOLDER\"] = points[:, 1].tolist()\n",
    "    placeholders[\"POINTS_Z_PLACEHOLDER\"] = points[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"R_PLACEHOLDER\"] = rgb[:, 0].tolist()\n",
    "    placeholders[\"G_PLACEHOLDER\"] = rgb[:, 1].tolist()\n",
    "    placeholders[\"B_PLACEHOLDER\"] = rgb[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"S_x_PLACEHOLDER\"] = 1\n",
    "    placeholders[\"S_y_PLACEHOLDER\"] = 1\n",
    "    placeholders[\"S_z_PLACEHOLDER\"] = 1\n",
    "\n",
    "    placeholders[\"CAMERA_X_PLACEHOLDER\"] = camera_position[0]\n",
    "    placeholders[\"CAMERA_Y_PLACEHOLDER\"] = camera_position[1]\n",
    "    placeholders[\"CAMERA_Z_PLACEHOLDER\"] = camera_position[2]\n",
    "\n",
    "    placeholders[\"LOOK_X_PLACEHOLDER\"] = look[0]\n",
    "    placeholders[\"LOOK_Y_PLACEHOLDER\"] = look[1]\n",
    "    placeholders[\"LOOK_Z_PLACEHOLDER\"] = look[2]\n",
    "\n",
    "    placeholders[\"AXIS_SIZE_PLACEHOLDER\"] = axis_size\n",
    "\n",
    "    placeholders[\"N_VOXELS_PLACEHOLDER\"] = sum(vector.reshape(-1) > 0)\n",
    "    \n",
    "    placeholders[\"FILENAME_PLACEHOLDER\"] = \"\\\"\" + filename + \"\\\"\"\n",
    "    placeholders[\"POINT_SIZE_PLACEHOLDER\"] = 0.3\n",
    "\n",
    "    \n",
    "    if output_name is None:\n",
    "        output_name = \"plotVG.html\"\n",
    "    \n",
    "    BASE_PATH = os.getcwd()\n",
    "    src = \"{}/{}\".format(BASE_PATH, \"obj_detection_plot.html\")\n",
    "    dst = \"{}/{}\".format(os.getcwd(), output_name)\n",
    "    point_size=0.001\n",
    "    \n",
    "    with open(src, \"r\") as inp, open(dst, \"w\") as out:\n",
    "        for line in inp:\n",
    "            for key, val in placeholders.items():\n",
    "                if key in line:\n",
    "                    line = line.replace(key, str(val))\n",
    "            out.write(line)\n",
    "\n",
    "    return IFrame(output_name, width=width, height=height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'car' b'lamp' b'table' b'airplane' b'chair']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"pyntcloud_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a444f4400>"
      ]
     },
     "execution_count": 767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b_label_ref.value)\n",
    "x = np.zeros((32,32,32))\n",
    "# plot_voxel_points(to_density(occ[1], upper=[32,32,32]), my_point_cloud.xyz)\n",
    "my_point_cloud.plot(point_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we need to train our CNN with partial data at least for a better performance, since it will always treat partial data as a full data, wanted to give some predictions even it hasn't seen that data before.\n",
    "\n",
    "Next, I will crop some corner, side point to train the neural net to identify is it a valid object or not.\n",
    "\n",
    "Meanwhile, below is the implementation for unsupervised region proposal to avoid more training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region Proposal\n",
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/NLeSC/PattyAnalytics/blob/master/patty/segmentation/dbscan.py\n",
    "\n",
    "from sklearn.cluster import dbscan\n",
    "def dbscan_labels(pointcloud, epsilon, minpoints, rgb_weight=0,\n",
    "                  algorithm='ball_tree'):\n",
    "    '''\n",
    "    Find an array of point-labels of clusters found by the DBSCAN algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pointcloud : pcl.PointCloud\n",
    "        Input pointcloud.\n",
    "    epsilon : float\n",
    "        Neighborhood radius for DBSCAN.\n",
    "    minpoints : integer\n",
    "        Minimum neighborhood density for DBSCAN.\n",
    "    rgb_weight : float, optional\n",
    "        If non-zero, cluster on color information as well as location;\n",
    "        specifies the relative weight of the RGB components to spatial\n",
    "        coordinates in distance computations.\n",
    "        (RGB values have wildly different scales than spatial coordinates.)\n",
    "    Returns\n",
    "    -------\n",
    "    labels : Sequence\n",
    "        A sequence of labels per point. Label -1 indicates a point does not\n",
    "        belong to any cluster, other labels indicate the cluster number a\n",
    "        point belongs to.\n",
    "    '''\n",
    "\n",
    "    if rgb_weight > 0:\n",
    "        X = pointcloud.to_array()\n",
    "        X[:, 3:] *= rgb_weight\n",
    "    else:\n",
    "        X = pointcloud\n",
    "\n",
    "    _, labels = dbscan(X, eps=epsilon, min_samples=minpoints,\n",
    "                       algorithm=algorithm)\n",
    "    return np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster_points(point_cloud, labels):\n",
    "    res = dict()\n",
    "    for idx, val in enumerate(labels):\n",
    "        if val not in res:\n",
    "            li = list()\n",
    "            li.append(point_cloud[idx])\n",
    "            res.update({val: li})\n",
    "        else:\n",
    "            li = res[val]\n",
    "            li.append(point_cloud[idx])\n",
    "            res.update( {val: li} )\n",
    "    \n",
    "    renew_dict = dict()\n",
    "    for idx, key in enumerate(res):\n",
    "        renew_dict.update({key: np.asarray(res[key])})\n",
    "    \n",
    "    return renew_dict\n",
    "\n",
    "\n",
    "def localize_region(point_cloud, target_cloud, grid_size=[32,32,32]):\n",
    "    \n",
    "    ((x_min, x_max), (y_min, y_max), (z_min, z_max)) = find_ranges(point_cloud)\n",
    "    ((x_target_min, x_target_max), (y_target_min, y_target_max), (z_target_min, z_target_max)) = find_ranges(target_cloud)\n",
    "    \n",
    "    x_length = (x_max - x_min)/32\n",
    "    y_length = (y_max - y_min)/32\n",
    "    z_length = (z_max - z_min)/32\n",
    "    \n",
    "    x_region_min = int(x_target_min/x_length)\n",
    "    x_region_max = int(x_target_max/x_length + 2)\n",
    "    \n",
    "    y_region_min = int(y_target_min/y_length)\n",
    "    y_region_max = int(y_target_max/y_length + 2)\n",
    "    \n",
    "    z_region_min = int(z_target_min/z_length)\n",
    "    z_region_max = int(z_target_max/z_length + 2)\n",
    "    \n",
    "    return ((x_region_min, x_region_max), (y_region_min, y_region_max), (z_region_min, z_region_max))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_cloud = norm_point(my_point_cloud.xyz)\n",
    "cluster_labels = dbscan_labels(normalized_cloud, 0.02, 10, algorithm='ball_tree')\n",
    "res = find_cluster_points(normalized_cloud, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a4328dcc0>"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = localize_region(normalized_cloud, res[0])\n",
    "den = np.zeros((32,32,32))\n",
    "den[region[0][0]:region[0][1], region[1][0]:region[1][1], region[2][0]:region[2][1]] = 1\n",
    "plot_voxel_points(den, my_point_cloud.xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxels = []\n",
    "for idx, key in enumerate(res):\n",
    "    if key != -1:\n",
    "        vox = voxelize3D(res[key], dim=[32,32,32])\n",
    "        vox_chan = np.array(vox).reshape(vox.shape + (1,))\n",
    "        voxels.append(vox_chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/shijian/git/3D-CNN/3d_pointcloud/trained_model/model-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/shijian/git/3D-CNN/3d_pointcloud/trained_model/model-2\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(voxels, output_format='probs', device_name='cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'car' b'lamp' b'table' b'airplane' b'chair']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00343599,  0.04454208,  0.00514553,  0.89686203,  0.05001442]], dtype=float32),\n",
       " array([[  3.52182433e-05,   8.41522455e-01,   9.86838937e-02,\n",
       "           1.22957979e-04,   5.96354268e-02]], dtype=float32),\n",
       " array([[ 0.02688141,  0.7284373 ,  0.13400495,  0.00968067,  0.10099573]], dtype=float32),\n",
       " array([[  1.96535271e-11,   2.92222392e-11,   9.63168191e-07,\n",
       "           6.79611640e-13,   9.99999046e-01]], dtype=float32),\n",
       " array([[  1.08488756e-07,   9.49013156e-06,   9.46136415e-01,\n",
       "           7.13847754e-08,   5.38539365e-02]], dtype=float32)]"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(b_label_ref.value)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# produce multi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(13368, 3)\n"
     ]
    }
   ],
   "source": [
    "my_point_cloud_a = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'airplane', 'points', '1a74b169a76e651ebc0909d98a1ff2b4.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "my_point_cloud_b = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'lamp', 'points', '1a9c1cbf1ca9ca24274623f5a5d0bcdc.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "my_point_cloud_c = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'car', 'points', '1a0c91c02ef35fbe68f60a737d94994a.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "my_point_cloud_d = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'chair', 'points', '1ab8a3b55c14a7b27eaeab1f0c9120b7.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "my_point_cloud_e = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'table', 'points', '1a1fb603583ce36fc3bd24f986301745.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "\n",
    "print(type(my_point_cloud_a.xyz))\n",
    "c = np.concatenate((my_point_cloud_a.xyz, my_point_cloud_b.xyz + 0.5, my_point_cloud_c.xyz + 1, my_point_cloud_d.xyz - 0.5, my_point_cloud_e.xyz - 1))\n",
    "print(c.shape)\n",
    "\n",
    "write_to_file(c, filename=\"ttt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"pyntcloud_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a2a1576d8>"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyntCloud.from_file(os.path.join(os.getcwd(), 'ttt2.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"]).plot(point_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ranges(point_cloud):\n",
    "\n",
    "    x_max = x_min = point_cloud[0][0]\n",
    "    y_max = y_min = point_cloud[0][1]\n",
    "    z_max = z_min = point_cloud[0][2]\n",
    "    \n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "        if coor[0] > x_max:\n",
    "            x_max = coor[0]\n",
    "        if coor[0] < x_min:\n",
    "            x_min = coor[0]\n",
    "        if coor[1] > y_max:\n",
    "            y_max = coor[1]\n",
    "        if coor[1] < y_min:\n",
    "            y_min = coor[1]\n",
    "        if coor[2] > z_max:\n",
    "            z_max = coor[2]\n",
    "        if coor[2] < z_min:\n",
    "            z_min = coor[2]\n",
    "            \n",
    "    return ((x_min, x_max), (y_min, y_max), (z_min, z_max))\n",
    "\n",
    "\n",
    "def find_cube_length(t_range, overall_size):\n",
    "    length = (t_range[1] - t_range[0])/overall_size\n",
    "    return length\n",
    "\n",
    "def get_all_length(point_cloud, overall_size, xyz_ranges):\n",
    "    \n",
    "    x_length = find_cube_length(xyz_ranges[0], overall_size[0])\n",
    "    y_length = find_cube_length(xyz_ranges[1], overall_size[1])\n",
    "    z_length = find_cube_length(xyz_ranges[2], overall_size[2])\n",
    "    \n",
    "    return (x_length, y_length, z_length)\n",
    "    \n",
    "def find_target_area_range(point_cloud, target_area, xyz_length, xyz_ranges):\n",
    "    \n",
    "    target_x_range_max = xyz_ranges[0][1] - target_area[0][0]*xyz_length[0]\n",
    "    target_x_range_min = xyz_ranges[0][1] - target_area[0][1]*xyz_length[0]\n",
    "    target_y_range_max = xyz_ranges[1][1] - target_area[1][0]*xyz_length[1]\n",
    "    target_y_range_min = xyz_ranges[1][1] - target_area[1][1]*xyz_length[1]\n",
    "    target_z_range_max = xyz_ranges[2][1] - target_area[2][0]*xyz_length[2]\n",
    "    target_z_range_min = xyz_ranges[2][1] - target_area[2][1]*xyz_length[2]\n",
    "    \n",
    "    return (target_x_range_min, target_x_range_max), (target_y_range_min, target_y_range_max), (target_z_range_min, target_z_range_max)\n",
    "\n",
    "\n",
    "def segment_points(point_cloud, target_ranges):\n",
    "    \"\"\"\n",
    "    Recives orig point cloud data and a tuple of boundary tuples like:\n",
    "        ((x_min, x_max), (y_min, y_max), (z_min, z_max))\n",
    "    \"\"\"\n",
    "    \n",
    "    target_x_range, target_y_range, target_z_range = target_ranges[0], target_ranges[1], target_ranges[2]\n",
    "    \n",
    "    points = []\n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "        if coor[0] >= target_x_range[0] and coor[0] <= target_x_range[1]:\n",
    "            if coor[1] >= target_y_range[0] and coor[1] <= target_y_range[1]:\n",
    "                if coor[2] >= target_z_range[0] and coor[2] <= target_z_range[1]:\n",
    "                    points.append(coor)\n",
    "                    \n",
    "    return np.asarray(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_cubes(point_cloud, overall_size=(16,16,16)):\n",
    "    \n",
    "    xyz_range = find_ranges(point_cloud)\n",
    "    xyz_length = get_all_length(point_cloud, overall_size, xyz_range)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
